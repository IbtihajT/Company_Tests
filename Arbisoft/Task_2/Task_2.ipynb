{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section consists of processing the raw data and store the data locally in image format. This markdown block consists of all the steps and the justification beind the steps.\n",
    "\n",
    "1. <strong>Random Seed</strong>\n",
    "\n",
    "  We first import the Numpy module and the we set a random seed for the reproducibility of the results.\n",
    "\n",
    "2. <strong>Load the json file</strong>\n",
    "\n",
    "  For loading the Json file, We're using python's built-in `json` moudle. We Specify the path to the json and then we use the `open` function to load the file. We read the data in a variable `raw_data` using the json's `load` method. Now the `raw_data` is of type `list` containing `dictionaries`.\n",
    "\n",
    "\n",
    "3. <strong>Process the Dictionaries inside the `raw_data`</strong>\n",
    "\n",
    "  For every dictionary object in the list, we extract some basic information like\n",
    "  - label\n",
    "  - color_mode\n",
    "  \n",
    "  After that we do 2 types of processing to the images, i.e if the `color_mode` is `gray` or if the `color_mode` is `R` (which corresponds to an RGB image).\n",
    "  \n",
    "  - In case of gray images, we first get the pixel values, Then we cast them to a numpy array, Then we calculate the reshapping index using the sqrt(len(pixel_array)). This will give us the number which can be utilized to reshape the image to a perfect square. For example sqrt(784) gives us 28 which tells us that the array can be reshaped to (28 x 28). We reshape the image and then we append it with the corresponding label to a list `data_in_list` in form of tuple. i.e (label, image) will be appended to the list.\n",
    "  \n",
    "  - In case of RGB images, we first get the pixel list for each 'R', 'G', 'B' channels, we then calculate the reshaping index for each channel individually using the same square_root methodology mentioned above and then reshape each channel. After that we use the Numpy's `dstack` method to merge the 3 channels in the format of BGR. Why BGR? Because by convention OpenCV loads the image in the format of BGR, whereas PIL loads the image in the format of RGB. I'm using OpenCV so i stacked them in the format of BGR just to stick with the convention. Then we again append the prcessed image in the list as mentioned above.\n",
    "  \n",
    "  - Now we have a list of tuples containing the label and the image. We first shuffle the list using the `shuffle` method belonging to `randome` module of python.\n",
    "  \n",
    "  - The next step is to create some directories. I'm storing the data in the format of keras's `ImageDataGenerator` which is discussed in detail in the next section. The directory structure becomes like\n",
    "  ```\n",
    "  data/images/\n",
    "    ├── test\n",
    "    └── train\n",
    "  ```  \n",
    "  \n",
    "  - Next we use the sklearn's `train_test_spilt` method which takes in the `data_in_list` and spits out 2 list named `train`, `test`. The `test_split` is 10% of the total data and the `random_state` is set as well for the reproduction of the split. Note that the data in `train` and `test` is the same as (label, image).\n",
    "  \n",
    "  - Now once we have our train and test lists, we can utilize them to save our data locally. We are using 2 loops (one for train and one for test) with the same logic. First we have a counter variable which we will use to name the imagew. Now for every label and image, we first check for if there is a directory named as label under the train directory. If there isn't one, we simply create one named the corresponding label. I'm using f-string literal for string formating. Once the directory is created, we simply save that image in the corresponding label directory using same f-string literal approach. At the end we increment the counter by 1. The following step generated a directory structure as follows\n",
    "  \n",
    "  `$tree -d ./data/images` report\n",
    "   ```\n",
    "   data/images/\n",
    "    ├── test\n",
    "    │   ├── Ankle boot\n",
    "    │   ├── automobile\n",
    "    │   ├── Bag\n",
    "    │   ├── bird\n",
    "    │   ├── Sandal\n",
    "    │   └── truck\n",
    "    └── train\n",
    "        ├── Ankle boot\n",
    "        ├── automobile\n",
    "        ├── Bag\n",
    "        ├── bird\n",
    "        ├── Sandal\n",
    "        └── truck\n",
    "   ```\n",
    "   \n",
    "   All the images are present in the corresponding directories. `-d` flag only reports directories and to be honest complete output of the tree will not fit the cell. Trust me on this one, The data is present in the directories :)\n",
    "   \n",
    "4) <strong>Class Weights Balancing</strong>\n",
    "    \n",
    "  Additionaly we will do one more step which is to calculate the class weights. Why are we calculating the class weights? The problem that i was facing was that the model wasn't able to classify some of the classes properly. When i analysed the problem, i found out that some of the classes like `automobile` contained only 443 samples in training set. So the data was heviely imbalanced. My solution to this problem was to calculate the `class_weights`. Model during its training time will be more bias to these classes and the problem of misclassifying the class will resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T11:38:17.086126Z",
     "start_time": "2020-02-06T11:38:17.079894Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T11:38:15.716885Z",
     "start_time": "2020-02-06T11:38:15.061392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import some required Libraries\n",
    "from os import makedirs, listdir\n",
    "from json import load\n",
    "from cv2 import imwrite\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:42.603456Z",
     "start_time": "2020-02-02T12:17:37.213878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to json file\n",
    "path = \"./data/gen_img_data.json\"\n",
    "\n",
    "# Load the json data\n",
    "with open(path) as json_file:\n",
    "    raw_data = load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:47.377927Z",
     "start_time": "2020-02-02T12:17:42.608945Z"
    }
   },
   "outputs": [],
   "source": [
    "data_in_list = list()\n",
    "\n",
    "# For every dictionary in `raw_data`\n",
    "for obj in raw_data:\n",
    "\n",
    "    # Get the Label of the Image\n",
    "    label = list(obj)[0]\n",
    "    # Get the Color_Mode of the image\n",
    "    color_mode = list(obj[label])[0]\n",
    "\n",
    "    # Case 1: If the image is gray\n",
    "    if color_mode == 'grey':\n",
    "\n",
    "        # Get the pixel List casted to numpy array\n",
    "        pixels = np.array(obj[label][color_mode])\n",
    "\n",
    "        # Reshape the 1D Array to a 2D array to form an image\n",
    "        reshape_index = int(sqrt(pixels.shape[0]))\n",
    "        image = np.reshape(pixels, (reshape_index, reshape_index))\n",
    "\n",
    "    # Case 2: If the image is Colored\n",
    "    elif color_mode == 'R':\n",
    "\n",
    "        # Get Red Channel, cast to numpy array and Reshape 1D -> 2D\n",
    "        r_pixels = np.array(obj[label]['R'])\n",
    "        reshape_index = int(sqrt(r_pixels.shape[0]))\n",
    "        r_pixels = np.reshape(r_pixels, (reshape_index, reshape_index))\n",
    "\n",
    "        # Get Green Channel, cast to numpy array and Reshape 1D -> 2D\n",
    "        g_pixels = np.array(obj[label]['G'])\n",
    "        reshape_index = int(sqrt(g_pixels.shape[0]))\n",
    "        g_pixels = np.reshape(g_pixels, (reshape_index, reshape_index))\n",
    "\n",
    "        # Get Blue Channel, cast to numpy array and Reshape 1D -> 2D\n",
    "        b_pixels = np.array(obj[label]['B'])\n",
    "        reshape_index = int(sqrt(b_pixels.shape[0]))\n",
    "        b_pixels = np.reshape(b_pixels, (reshape_index, reshape_index))\n",
    "\n",
    "        # Combine the 3 channels\n",
    "        image = np.dstack((b_pixels, g_pixels, r_pixels))\n",
    "\n",
    "    # Append the label and image in the list\n",
    "    data_in_list.append((label, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:47.402714Z",
     "start_time": "2020-02-02T12:17:47.379724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Class Counts:\n",
      "\n",
      "{'Ankle boot': 6000,\n",
      " 'Bag': 6000,\n",
      " 'Sandal': 1201,\n",
      " 'automobile': 501,\n",
      " 'bird': 5000,\n",
      " 'truck': 5000}\n"
     ]
    }
   ],
   "source": [
    "# Some statistics\n",
    "\n",
    "# We use `list comprehension` to get the labels from the `data_in_list` and we use\n",
    "# numpy's `unique` method to calculate the count of the class\n",
    "total_class_count = np.unique(\n",
    "    [data_point[0] for data_point in data_in_list], return_counts=True)\n",
    "print(\"Total Class Counts:\\n\")\n",
    "pprint(dict(zip(total_class_count[0], total_class_count[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:47.442327Z",
     "start_time": "2020-02-02T12:17:47.408027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the data list using the seed for reproduction of the results\n",
    "random.Random(42).shuffle(data_in_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:47.448161Z",
     "start_time": "2020-02-02T12:17:47.444615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make \"images\", \"train\", \"test\" directories\n",
    "try:\n",
    "    makedirs(\"./data/images\")\n",
    "    makedirs(\"./data/images/train\")\n",
    "    makedirs(\"./data/images/test\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:47.467046Z",
     "start_time": "2020-02-02T12:17:47.449981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a train_test split of the data with test_size of 10%. We also set a random state to reproduce the split\n",
    "train, test = train_test_split(data_in_list, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:47.494534Z",
     "start_time": "2020-02-02T12:17:47.470040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Counts:\n",
      "\n",
      "{'Ankle boot': 5424,\n",
      " 'Bag': 5408,\n",
      " 'Sandal': 1065,\n",
      " 'automobile': 444,\n",
      " 'bird': 4494,\n",
      " 'truck': 4496}\n",
      "\n",
      "\n",
      "Test Counts:\n",
      "\n",
      "{'Ankle boot': 576,\n",
      " 'Bag': 592,\n",
      " 'Sandal': 136,\n",
      " 'automobile': 57,\n",
      " 'bird': 506,\n",
      " 'truck': 504}\n"
     ]
    }
   ],
   "source": [
    "# Check train, test statistics\n",
    "train_class_count = np.unique([data_point[0]\n",
    "                               for data_point in train], return_counts=True)\n",
    "test_class_count = np.unique([data_point[0]\n",
    "                              for data_point in test], return_counts=True)\n",
    "\n",
    "print(\"Train Counts:\\n\")\n",
    "pprint(dict(zip(train_class_count[0], train_class_count[1])))\n",
    "\n",
    "print(\"\\n\\nTest Counts:\\n\")\n",
    "pprint(dict(zip(test_class_count[0], test_class_count[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:49.298508Z",
     "start_time": "2020-02-02T12:17:47.497351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the train files locally based on the label\n",
    "\n",
    "# Counter is used for naming of the image files\n",
    "counter = 1\n",
    "\n",
    "# For every label and image in the train set\n",
    "for label, image in train:\n",
    "\n",
    "    # Check if the label directory exists under the train directory\n",
    "    try:\n",
    "        # Make the label directory under train\n",
    "        makedirs(f\"./data/images/train/{label}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Write the image under the label directory\n",
    "    imwrite(f\"./data/images/train/{label}/{counter}.jpg\", image)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:49.523536Z",
     "start_time": "2020-02-02T12:17:49.300613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the test files locally based on the label\n",
    "\n",
    "# Counter is used for naming of the image files\n",
    "counter = 1\n",
    "\n",
    "# For every label and image in test set\n",
    "for label, image in test:\n",
    "\n",
    "    # Check if the label directory exists under the test directory\n",
    "    try:\n",
    "        # Make the label directory under test\n",
    "        makedirs(f\"./data/images/test/{label}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Write the image under the label directory\n",
    "    imwrite(f\"./data/images/test/{label}/{counter}.jpg\", image)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:49.554010Z",
     "start_time": "2020-02-02T12:17:49.526060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class balancing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Get the unique classes\n",
    "classes = np.array(listdir(\"./data/images/train/\"))\n",
    "\n",
    "# Calculate the class_weights\n",
    "weights = compute_class_weight('balanced', classes, np.array(\n",
    "    [data_point[0] for data_point in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:17:49.569864Z",
     "start_time": "2020-02-02T12:17:49.555617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ankle boot': 0.655451081612586,\n",
      " 'Bag': 0.6573902859960552,\n",
      " 'Sandal': 3.3381846635367762,\n",
      " 'automobile': 8.007132132132131,\n",
      " 'bird': 0.7910918261385551,\n",
      " 'truck': 0.7907399169632265}\n"
     ]
    }
   ],
   "source": [
    "# Check the calculated weights and cast them to a dictionary for now\n",
    "weights = dict(zip(classes, weights))\n",
    "pprint(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the weights calculated based on the number of samples per class in the train set. The summary is as follows\n",
    "\n",
    "| Class      | Training Samples | Calculated weights |\n",
    "|------------|------------------|--------------------|\n",
    "| Ankle boot |       5424       |        0.66        |\n",
    "| Bag        |       5408       |        0.66        |\n",
    "| Sandal     |       1065       |        3.34        |\n",
    "| automobile |        444       |        8.01        |\n",
    "| bird       |       4494       |        0.79        |\n",
    "| truck      |       4496       |        0.79        |\n",
    "\n",
    "We can clearly see that the automobile class is assigned with the heaviest weight amont all others because of the less number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Image Augmentation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, i'm going to use the keras's `ImageDataGenerator` module. I'm using this module for multiple reasons as it will help me in the following\n",
    "\n",
    "  - Efficient on the memory as the images will be loaded to the RAM in batches.\n",
    "  - Easy to use augmentation as it includes functions like \n",
    "      - Rescaling\n",
    "      - Horizontal and Vertical Flip\n",
    "      - Random Rotation\n",
    "  - Train/Validation Split\n",
    "  \n",
    "1. <strong>Get the file path and get the classes</strong>\n",
    "   \n",
    "   We first specify the train directory path and the test directory path. Then we use one of the directories to capture the unqie classes using os module's `listdir` which will be used later. \n",
    "   \n",
    "   \n",
    "2. <strong>Create a train_datagen object</strong>\n",
    "   \n",
    "   I'm creating an ImageDataGenerator, `train_datagen` object where i specify the object to\n",
    "     - Rescale the images\n",
    "     - Apply Horizontal flip Augmentation\n",
    "     - Apply Vertical Flip Augmentation\n",
    "     - Apply Random Rotation Augmentation with thresh of 0.2\n",
    "     - Validation Split of 25%\n",
    "   \n",
    "   Note that i'm only using augmentation techniques that make sense. For example it doesn't make any sense to apply Vertical Flip to MNIST Dataset.\n",
    "   \n",
    "   Using the above created `ImageDataGenerator` object, i use the `flow_from_directory` method in which i specify the \n",
    "   \n",
    "   - Path to the data\n",
    "   - <strong>target_size</strong> I have 2 target size as of (28, 28) and (32, 32) so i have used an average target size of (30, 30). I have not used anyother size as using a large size will blur the images and using a very small size will effect the training as well.\n",
    "   - <strong>batch_size</strong> I'm using a batch size of 32 for training and 16 for validation as i have a low_end GPU (GeForce 940M)\n",
    "   - <strong>class_mode</strong> is set to `categorical`\n",
    "   - <strong>classes</strong> is set to the `classes` variable containing the classes.\n",
    "   - <strong>subset</strong> is set to `training` in case of the `train_generator` and `validation` in case of the `validation_generator`.\n",
    "   - <strong>color_mode</strong> is set to `grayscale` as i will be training the model on grayscale images.\n",
    "   \n",
    "   Note that there is a difference between train, validation and test. `Validation` data is a subset of the `train` data and we use it for the validation of the model's performance where as `Test` data is used for model's evaluation.\n",
    "   \n",
    "   \n",
    "   <div style=\"text-align:center\"><img src=\"https://miro.medium.com/max/776/1*Nv2NNALuokZEcV6hYEHdGA.png\" width=\"50%\" height=\"25%\" /></div>\n",
    "   \n",
    "   Finally we create a seperate generator for test set in which we only rescale the image. This Generator will be used later on in the evaluation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:18:49.319034Z",
     "start_time": "2020-02-02T12:18:47.299606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required Libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:18:49.383410Z",
     "start_time": "2020-02-02T12:18:49.380115Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = \"./data/images/train/\"\n",
    "test_path = \"./data/images/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:18:51.500262Z",
     "start_time": "2020-02-02T12:18:51.481428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create ImageDataGenerator Object\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # To rescale between (0, 1)\n",
    "    horizontal_flip=True,  # Will apply horizontal augmentation\n",
    "    vertical_flip=True,  # Will apply  vertical augmentation\n",
    "    rotation_range=0.2,  # Will apply rotation augmentation\n",
    "    validation_split=0.25  # Will make a Validation Split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:18:56.141865Z",
     "start_time": "2020-02-02T12:18:55.263025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15999 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create train_generator\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,  # Path to data\n",
    "    target_size=(30, 30),  # Average target size (32 + 28)/2 = 30\n",
    "    batch_size=32,  # Batch size\n",
    "    class_mode='categorical',  # Categorical Class mode\n",
    "    classes=list(classes),  # Classes\n",
    "    subset='training',  # Training Subset\n",
    "    color_mode='grayscale',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:19:02.513392Z",
     "start_time": "2020-02-02T12:19:01.961793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5332 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create validation_generator\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_path,  # Path to data\n",
    "    target_size=(30, 30),  # Average target size\n",
    "    batch_size=16,  # Batch Size\n",
    "    class_mode='categorical',  # Categorical Class mode\n",
    "    classes=list(classes),  # Classes\n",
    "    subset='validation',  # Validation Subset\n",
    "    color_mode='grayscale',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:19:15.741761Z",
     "start_time": "2020-02-02T12:19:15.501336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2371 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_path,  # Path to data\n",
    "    target_size=(30, 30),  # Average target size (32 + 28)/2 = 30\n",
    "    batch_size=8,  # Batch size\n",
    "    class_mode='categorical',  # Categorical Class mode\n",
    "    classes=list(classes),  # Classes\n",
    "    color_mode='grayscale'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:19:25.112279Z",
     "start_time": "2020-02-02T12:19:25.091249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if the class indices are equal for every split\n",
    "train_generator.class_indices == validation_generator.class_indices == test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:19:35.173261Z",
     "start_time": "2020-02-02T12:19:35.169220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the class indice with the corresponding weight value\n",
    "# We will use dictionary for this purpose to get the class indices w.r.t class_weight\n",
    "class_weights = {train_generator.class_indices[key]: weights[key]\n",
    "                 for key in train_generator.class_indices.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:19:39.588129Z",
     "start_time": "2020-02-02T12:19:39.578913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6573902859960552,\n",
       " 1: 3.3381846635367762,\n",
       " 2: 8.007132132132131,\n",
       " 3: 0.7910918261385551,\n",
       " 4: 0.7907399169632265,\n",
       " 5: 0.655451081612586}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class weights with class indices\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to build a model and train is using the data we processed above.\n",
    "\n",
    "1. We build a custome `Squential` model which consists of 3 Convolutional blocks for feature extraction.\n",
    "\n",
    "  - The First Convolutional block contains a `Conv2D` layer with 128 kernels of size (3 x 3). We use `relu` as the activation function. After that we apply `MaxPooling2D` layer with size (2 x 2) to reduce the image size which can help prevent the `Curse Of Dimentionality`.\n",
    "  \n",
    "  - The Second Convolutional block contains a `Conv2D` layer with 64 kernels of size (3 x 3). We use `relu` as the activation function. After that we apply `MaxPooling2D` layer with size (2 x 2) for the same reason.\n",
    "  \n",
    "  - The third Convolutional block contains a `Conv2D` layer with 32 kernels of size (3 x 3). We use `relu` as the activation function. After that we apply `MaxPooling2D` layer with size (2 x 2).\n",
    "  \n",
    "  - We then use the `Flatten` layer to convert to 1D Vector.\n",
    "  \n",
    "  - We have a `Dense` layer with 128 neurons with an activation of `relu`\n",
    "  \n",
    "  - We then use a `Dropout` layer for regularization.\n",
    "  \n",
    "  - We then have a Classification `Dense` Layer of 6 neurons (Number of classes) with an activation of `softmax` which will return the probability vector containing probabilities of each class.\n",
    "  \n",
    "  - We then compile the model with `loss=categorical_crossentropy` and `optimizer=adam`.\n",
    "  \n",
    "  - We are also generating the `callbacks` list for the purpose of saving the best weights.\n",
    "  \n",
    "  - We check for if there is a `checkpoints` directory. If there is no directory, we create one and we save the models in that directory in `hdf5` format.\n",
    "  \n",
    "2. We are going to use the `fit_generator` method as we have the data in the format of `ImageDataGenerator`.\n",
    "\n",
    "  - We save the history as the model is being trained. \n",
    "  \n",
    "  - `steps_per_epoch` is set to `train_data.n // train_data.batch_size`. This is a formal way to chose a step size. We chose the ratio of total number of samples and the size of the batch size.\n",
    "  \n",
    "  - `validation_steps` follows the same strategy of `validation_generator.n // validation_generator.batch_size`.\n",
    "  \n",
    "  - We then used the `history` to plot the `training_accuracy` against `validation_accuracy` and `training_loss` against `validation_loss` to see if the model is overfitting or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T11:38:31.875514Z",
     "start_time": "2020-02-06T11:38:30.259090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libararies\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:21:07.736188Z",
     "start_time": "2020-02-02T12:21:07.721363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a custom model\n",
    "\n",
    "\n",
    "def my_model(input_shape, num_classes):\n",
    "\n",
    "    # We will use Sequential Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(128, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Classification Block\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add regularization\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Create checkpoints directories\n",
    "    try:\n",
    "        # If directory not found\n",
    "        makedirs(f\"./checkpoints\")\n",
    "    except Exception as e:\n",
    "        # If directory already exists\n",
    "        pass\n",
    "\n",
    "    # Define Model checkpoints and callbacks_list\n",
    "    filepath = \"./checkpoints/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [EarlyStopping(\n",
    "        monitor='val_accuracy', patience=5), checkpoint]\n",
    "\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:30:56.722641Z",
     "start_time": "2020-02-02T12:30:56.473773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the model and callbacks_list\n",
    "model, callbacks_list = my_model(train_generator.image_shape, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T12:21:13.981274Z",
     "start_time": "2020-02-02T12:21:13.953596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 128)       1280      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 32)          18464     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 6150      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 789,606\n",
      "Trainable params: 789,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:17:46.666786Z",
     "start_time": "2020-02-02T12:32:08.751399Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.7314 - accuracy: 0.6901\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85454, saving model to ./checkpoints/weights-improvement-01-0.85.hdf5\n",
      "499/499 [==============================] - 167s 334ms/step - loss: 0.7304 - accuracy: 0.6905 - val_loss: 0.4485 - val_accuracy: 0.8545\n",
      "Epoch 2/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7848\n",
      "Epoch 00002: val_accuracy improved from 0.85454 to 0.88326, saving model to ./checkpoints/weights-improvement-02-0.88.hdf5\n",
      "499/499 [==============================] - 137s 274ms/step - loss: 0.5125 - accuracy: 0.7851 - val_loss: 0.3455 - val_accuracy: 0.8833\n",
      "Epoch 3/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.8174\n",
      "Epoch 00003: val_accuracy did not improve from 0.88326\n",
      "499/499 [==============================] - 139s 278ms/step - loss: 0.4629 - accuracy: 0.8177 - val_loss: 0.3312 - val_accuracy: 0.8741\n",
      "Epoch 4/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.8358\n",
      "Epoch 00004: val_accuracy improved from 0.88326 to 0.88833, saving model to ./checkpoints/weights-improvement-04-0.89.hdf5\n",
      "499/499 [==============================] - 130s 260ms/step - loss: 0.4306 - accuracy: 0.8360 - val_loss: 0.3147 - val_accuracy: 0.8883\n",
      "Epoch 5/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8485\n",
      "Epoch 00005: val_accuracy improved from 0.88833 to 0.90240, saving model to ./checkpoints/weights-improvement-05-0.90.hdf5\n",
      "499/499 [==============================] - 131s 262ms/step - loss: 0.4081 - accuracy: 0.8487 - val_loss: 0.2901 - val_accuracy: 0.9024\n",
      "Epoch 6/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8558\n",
      "Epoch 00006: val_accuracy did not improve from 0.90240\n",
      "499/499 [==============================] - 130s 261ms/step - loss: 0.3972 - accuracy: 0.8559 - val_loss: 0.3618 - val_accuracy: 0.8590\n",
      "Epoch 7/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.3680 - accuracy: 0.8616\n",
      "Epoch 00007: val_accuracy did not improve from 0.90240\n",
      "499/499 [==============================] - 131s 262ms/step - loss: 0.3682 - accuracy: 0.8617 - val_loss: 0.3232 - val_accuracy: 0.8527\n",
      "Epoch 8/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.8674\n",
      "Epoch 00008: val_accuracy did not improve from 0.90240\n",
      "499/499 [==============================] - 130s 261ms/step - loss: 0.3493 - accuracy: 0.8675 - val_loss: 0.2701 - val_accuracy: 0.8958\n",
      "Epoch 9/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8777\n",
      "Epoch 00009: val_accuracy improved from 0.90240 to 0.90897, saving model to ./checkpoints/weights-improvement-09-0.91.hdf5\n",
      "499/499 [==============================] - 141s 282ms/step - loss: 0.3342 - accuracy: 0.8777 - val_loss: 0.2479 - val_accuracy: 0.9090\n",
      "Epoch 10/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8815\n",
      "Epoch 00010: val_accuracy improved from 0.90897 to 0.91348, saving model to ./checkpoints/weights-improvement-10-0.91.hdf5\n",
      "499/499 [==============================] - 139s 279ms/step - loss: 0.3290 - accuracy: 0.8818 - val_loss: 0.2263 - val_accuracy: 0.9135\n",
      "Epoch 11/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8931\n",
      "Epoch 00011: val_accuracy did not improve from 0.91348\n",
      "499/499 [==============================] - 138s 276ms/step - loss: 0.3093 - accuracy: 0.8932 - val_loss: 0.2609 - val_accuracy: 0.8981\n",
      "Epoch 12/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8949\n",
      "Epoch 00012: val_accuracy did not improve from 0.91348\n",
      "499/499 [==============================] - 140s 280ms/step - loss: 0.2953 - accuracy: 0.8948 - val_loss: 0.2633 - val_accuracy: 0.8943\n",
      "Epoch 13/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8933\n",
      "Epoch 00013: val_accuracy did not improve from 0.91348\n",
      "499/499 [==============================] - 140s 280ms/step - loss: 0.2935 - accuracy: 0.8930 - val_loss: 0.5565 - val_accuracy: 0.7793\n",
      "Epoch 14/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2848 - accuracy: 0.8957\n",
      "Epoch 00014: val_accuracy did not improve from 0.91348\n",
      "499/499 [==============================] - 140s 281ms/step - loss: 0.2846 - accuracy: 0.8957 - val_loss: 0.3090 - val_accuracy: 0.9062\n",
      "Epoch 15/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.9078\n",
      "Epoch 00015: val_accuracy improved from 0.91348 to 0.92812, saving model to ./checkpoints/weights-improvement-15-0.93.hdf5\n",
      "499/499 [==============================] - 139s 279ms/step - loss: 0.2755 - accuracy: 0.9079 - val_loss: 0.2050 - val_accuracy: 0.9281\n",
      "Epoch 16/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8970\n",
      "Epoch 00016: val_accuracy did not improve from 0.92812\n",
      "499/499 [==============================] - 140s 280ms/step - loss: 0.2784 - accuracy: 0.8971 - val_loss: 0.2313 - val_accuracy: 0.9257\n",
      "Epoch 17/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2799 - accuracy: 0.8983\n",
      "Epoch 00017: val_accuracy did not improve from 0.92812\n",
      "499/499 [==============================] - 133s 267ms/step - loss: 0.2796 - accuracy: 0.8985 - val_loss: 0.2347 - val_accuracy: 0.9129\n",
      "Epoch 18/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9067\n",
      "Epoch 00018: val_accuracy did not improve from 0.92812\n",
      "499/499 [==============================] - 132s 264ms/step - loss: 0.2588 - accuracy: 0.9067 - val_loss: 0.3641 - val_accuracy: 0.8442\n",
      "Epoch 19/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.9100\n",
      "Epoch 00019: val_accuracy did not improve from 0.92812\n",
      "499/499 [==============================] - 131s 263ms/step - loss: 0.2476 - accuracy: 0.9097 - val_loss: 0.2285 - val_accuracy: 0.9125\n",
      "Epoch 20/50\n",
      "498/499 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9144\n",
      "Epoch 00020: val_accuracy did not improve from 0.92812\n",
      "499/499 [==============================] - 131s 263ms/step - loss: 0.2410 - accuracy: 0.9144 - val_loss: 0.2375 - val_accuracy: 0.9125\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.n // validation_generator.batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    class_weight=class_weights,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:36:44.241501Z",
     "start_time": "2020-02-02T13:36:43.817303Z"
    }
   },
   "outputs": [],
   "source": [
    "# For plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:36:44.781741Z",
     "start_time": "2020-02-02T13:36:44.298046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXxU1fn48c+Tfd/ZAwRUdmRHwA1EWdS6VIuKWK3WtVq11Z9i1aq1rf3aWvcNpVoVN9SKSiuioCj7JrIawEAWCBDIQvbl/P44NzCEJEzIbEme9+s1r5m599w7z0wm89xz7rnniDEGpZRSKtAE+TsApZRSqj6aoJRSSgUkTVBKKaUCkiYopZRSAUkTlFJKqYCkCUoppVRA0gSlVBOJyGsi8qibZTNE5Gxvx6RUa6QJSimlVEDSBKVUGyciIf6OQan6aIJSrZbTvHa3iKwTkWIReVVEOojIf0WkSETmi0iiU/YCEdkgIvkislBE+rrsZ4iIrHa2eReIqPM654vIWmfbxSJychPjHCkiS5ztd4nIsyIS5rK+v4h8ISL7RSRXRO5zlgeLyH0iss2JbZWIdBWRNBExronHeU+/dh5fIyLficg/RWQ/8JCInCAiX4lInojsE5G3RCTBZfuuIvKhiOx1yjwrIuFOTANdyrUXkVIRadeUz0Cp+miCUq3dJcA5QC/gZ8B/gfuAFOz3/7ci0gt4G7gDaAfMBT4RkTAnUfwHeANIAt539gmAiAwFZgI3AsnAS8AcEQlvQozVwJ1OTKOB8cAtzv5jgfnA/4DOwInAl852vwOuAM4F4oBrgRI3X/MUYDvQHvgzIMBfndfoC3QFHnJiCAY+BXYAaUAX4B1jTDnwDjDNZb9XAPONMXvdfvdKNUATlGrtnjHG5BpjsoFFwDJjzBrnx/UjYAhwGfCZMeYLY0wl8HcgEhgDjAJCgSeNMZXGmNnACpf9Xw+8ZIxZZoypNsa8DpQ727nFGLPKGLPUGFNljMnAJrkzndXnA7uNMf8wxpQZY4qMMcucdb8G7jfGbDHW98aYPDdfNscY84zzmqXGmK3O+y93kssTLjGMxCauu40xxU4c3zrrXgemikjtb8lV2GSuVLNp27Nq7XJdHpfW8zwG++O7o3ahMaZGRDKxNYVqINscOaryDpfH3YGrReQ2l2Vhzj7d4tTgngCGA1HY/8tVzuquwLYGNm1s3bFk1omhPfA0cDoQiz14PeDyOjuMMVV1d2KMWSYixcCZIrILW8Obc5wxKXUErUEpBTnYRAOAiAj2Rzkb2AV0cZbV6ubyOBP4szEmweUWZYx5uwmv/wKwGTjJGBOHbYKsfb1M4IQGtmtoXbFzH+WyrGOdMnWnMfirs+xkJ4ZpdWLo1khnited8lcBs40xZQ2UU6pJNEEpBe8B54nIeBEJBX6PbaZbDCwBqrDnqkJE5OfYJq9aM4CbROQUsaJF5Dzn3JG7YoFC4KCI9AFudln3KdBRRO5wOiXEisgpzrpXgD+JyEnOa58sIslOE102MM3pSHEtDSc51xgOAvki0gW422Xdcmyifsx5fxEicqrL+jeAi7FJ6t9NeN9KNUoTlGrzjDFbsD+uzwD7sJ0pfmaMqTDGVAA/B67BNnldBnzosu1K7HmoZ531W52yTXEXMBUowia8d132X4Tt5PEzYDeQDoxzVj+BTa7zsAnuVey5M5yY7gbygP7YZNuYh4GhQAHwWZ33WO28/onATiAL+znUrs8CVmNrYIua8L6VapTohIVKqeYSkZnYjhf3+zsW1XpoJwmlVLOISBq2ljnEv5Go1kab+JTyAefi4IP13O7zd2zNISJ/AtYDjxtjfvJ3PKp10SY+pZRSAUlrUEoppQJSizsHlZKSYtLS0vwdhlJKKQ9ZtWrVPmPMUeM3trgElZaWxsqVK/0dhlJKKQ8RkR31LdcmPqWUUgGpxdWglFKqUQf3wrp3IXslhERAaBSERUFoNIRGQlh0Pcucx67LQiPhiBGulK9pglJKtXzVlZD+Bax9C378H9RUQWIa1NRAZTFUlEBVaRN3KjaRxbSH1OGQOtLedxwIwaHeeBeqDk1QSqmWa88mWPMmrHsPivdAdHsYdTMMngbt+xxZtqbGJqmKksNJq9K5NbisBA5kwE+L4If37X5CIqDzEEgdYW9dR0Js3bF4lSdoglJKtSyl+bD+A1tbyl4FQSHQaxIMmQYnnt1w7SYoyDbvhUVj56VsAmOgIAuylkPWSshcDktfgJqn7fr4rkcmrI4DIaQpc1aq+miCUspXjIGKYijJg+gU54dSuaWmBn762ialTZ9AVRm07w8T/wIDp0CMl2eYF4GErvY2wJlQubIMdq+DrBU2YWUuhw3OGLvB4dBpkE1Wtc2D8V28G6OXlVdVc6C4krzicvYXVxy6nX5SCie2b8rg/e5rcSNJDB8+3Gg3cxUQKkttsjl02+/c8upZ7jyuLrfbBoXaH6+e46DnWNtkFKzHi0fZ/xOsnQXfvw0FmRARDwN/AYOvtJ9ZoHViKMw5nLCyVkLOmsN/85MmwuVv+f/8VekBzOxrMXnbqakxVBtz6L66Bmpcnh++t8vrkzX8Xk694LpmhSQiq4wxw49arglKqUZUV9qj5MzlkLkM9m8/nHAqSxreLjIRopJdbkmHH0cmQt422L4Qdn0PGAiPg7TTbbLqORZSTgq8H19fqSiGjXNsbSljESBwwjjbhNf7PAiNaHTz/JIKvs8qYE9hGb07xtKrQywRocG+ib2uqgrI/QG2/Be+eRyGXQPnP3ncf9uyymoOlFRQXF7FwfJq576KYud21LKKw8uKy6soLSvj/8oeZojZxNyaU6ip50qjYIGwkGDCQ4IICwk64j48JLjOsmBCRvyK4B6n1hOt+xpKUF49ZBORScBTQDDwijHmsTrruwMzsQ3C+4FpztwySvlHyX4nGS2199mrD/f+iu8K7fvapqVDCSepTiJKhogE92tDxXmQ8Y1NVtsXwpbP7PK4LoeTVY8zIbaDp99pYMpcAW/+HMoLIbEHnHU/DLoC4lPrLV5WWc2GnALWZhbwfWY+32flsyPvyAOH4CChZ0o0/TrH0a9THP06x9G3UxwpMT44RxQSBl2G2VtNNXz7BKT0htG3NLhJWWU1O/eX8NO+YjL2FZORV0LGvmJ25BWTU3DsyYqDBKLDQ4gJDyHaucWEB5McFcm1B17klLL1fHrCH8ntfiGJ0WEkR4eRFB1GcnQ4STFhRIcFIwFycOS1GpSIBAM/YidbywJWAFcYYza6lHkf+NQY87qInAX8yhhzVWP71RqUF1SU2GaJHYshLx1OuRm6jvB3VN5XU2Pf786lh2tIeel2XVCIcw7hFOc8go/OIez/yZ5r2bbA3pcesMvb9zucsLqPgXDvtPn73b8vtD3zLv2XfZ8uP5TVNYb0PUV8n5l/KCFtyS2iusb+hnWOj+Dk1AQGdU1gUNd4OsRFkJ5bxMacQjbuKmRjTuERP/DtY8MPJavaxJWWHE1wkJd+nGtq4P1fwqZPqZjyNhnJp7mVhJKiw0hLjiItOZruydG0iw0nOjz4UAKKcUlGMeEhRIQG1Z9gFj8D8+6H038P4x/0zns8Tj5v4hOR0cBDxpiJzvPpAMaYv7qU2QBMNMZkif1EC4wxcY3tVxOUB5QV2h/kHd/apJS9GmoqQYIgLNZ2tz37IRh9a+tqZqootr2+MpcdPqldlm/XRSYdTkZdT7HnN8Ki/BtvTY1tXqytXe1cYjsHBIXY3mI9x0LSCRCZYM/NRCQcftwSe5DtWgcvnQ7j/4g57U6yDpTyfVa+rRllFvBDdgGlldUAxEWE2ERUm5BS42kf13jTH9jmv9pktXFXIZt2FZGeW0SVk+QiQ4Pp3TH2UG2rb6c4UhMjqaiqobK6hspqQ2V1DRXVNVRW2ecV1dVUVBlnfY2z3lBZ5Vquhr0HK9i1J497c+8ktSaHSyoeYovpBtgk1D05ih5OEkpLiaJHin0cH+mBc1ab58I7U6HfBXDpa7ZHYwDxR4K6FJhkjPm18/wq4BRjzK0uZWYBy4wxT4nIz4EPgBRjTF6dfd0A3ADQrVu3YTt21Dtsk2pIyX7747ZjMWR8a3/0TI39oes81B6ppp1mf5yNgTm32p5SvSbDRc/bZqyWyhhY8waseBV2/wDG/sDRro+TkJxb8gmBn4wry2zTY23CylmLnWX9aCYkkprwOGrC46kOi6MqLJ6K0DgqQ2OpCI2jPDiW0pBYSoNiOBiaTFHKYMJDg484xxAeEkx46JHnG8KddSHBx/6Bq6yuoaS8muKKKkoqqiiufezcl1ZUU1xRTUl5FcUV1Uz+8QF6Fyzizs6zWLarmv3FFQCEhQTRv3Mcg1ITGNzVJqS05CiPNUOVV1Wzdc9BNu2qrW0VsDGnkMKyKo/sv1ZiVChpKdEMjivh9ztvJig4hJ8u/oTU1DTio7zYcWLXOpg5Cdr1hms+8/+BVz38kaB+ga0duSaokcaY21zKdAaeBXoA3wCXAP2NMQUN7VdrUG4oyoUd39mEtGMx7NlglweH2yPvtFNtUkodUX9XZ2Ng+cvw+R/sBYiX/qtlNvmVFcKnd9hrZjoNhpPOsckodbjtqNDCFJVVsjGnkPU5hazPLiAjexehpblEVB0ksrqIqJqDRJti4ikmTux9vBQTRwnxcnh5LKUEyZH/93+tvIKXqn/mdixBwhEJLDwkmJBgobyy5lASqqiucXt/3UP282XIb5kdfB6vx93IgM5xDOpqE1KvDrGEhfj2iN8YQ05BGRuyC9h7sJywYJukQ4Nrb0JYcBChIYefhx+xPshZL4QGBxESJEcm1Jw1MHMydBwAV396zI4fx61wF8w4yx58Xf9VwF5QHJBNfHXKxwCbjTH1nw11aIKqR36mk4ycJru8rXZ5aDR0O8Umo+6nQZehTWv6yV4N718Dhdktr8lv1/c29gMZMO4PcNrvAq5ZozEFpZVsyC5gfU4BP2QXsiG7gO37ig+t7xAXTv/O8bSLCSesTm+rsBD74xgeGkx4cNDR68UQaUqIrD5IRHURKUv/QtiedWydupjSoGjKK21yKa+sobyqhvKqaiqqDj+uXW7LVDvL7fPI0GCiw4KJCg+x92EhRIcHExl25HPX+6iwYELnP2AvfL19LSR08+Mn70Mb58B7V8GAS+GSVzz/v1VRAv+aDPvS4brP7cXDAcofvfhWACeJSA8gG7gcmFonqBRgvzGmBpiO7dGnGmMM5O+AjO9sLSnjW/sc7LmHbmNg6NXQ/VTodHLzrrnoMhRu/MY2+c27375moDf5GQMrXoHP77M96q7+1NYYA9j+4grWO8lofXYB67ML2bn/cE+0zvERDOgSz0VDujCwSzz9u8TRPtaDR9wJD8GMs+i9Yxaccbfn9uuu0nxY9RoM+HnbSU5gzweNfxC+fMQ2v535/zy375oa+OgGe6B2xdsBnZwa47UEZYypEpFbgc+x3cxnGmM2iMgjwEpjzBxgLPBXETHYJr7feCsej8pZA98+aU+wp/Syt3a9bffRmPaePRIyxl57k/Gtk5C+g0KnJ35kkq0djbrZJqQO/SHIw9d7RCbAlDcON/m9dEbgNvmVFcCc22Djx3bIm4tfsiM2+JExhpKKavJLK8kvqaCgtJL8kkq27TnID9kFbMgpJDv/8CCmXZMiGdglnstGdLXJqHMcyd7uDt1lmB0qaPGzMPIGe6DjS6v+BRUHYcxtxy7b2pz2O1vDWfBnSD7RJmlP+OoRex554l+g92TP7NMP9ELdpsheDV//zY6WHBEPST3tl6vi4OEyEfFO0uptL7Zs19s+T0xzL3kYY/e549vDtaSiXXZdVIpz/ug026mhXR/fNlsFcpNfzhobW34mjH8Axtzu8c+mtKKa3YVlhxJNbbLJL6kkv9RZVlJ5RDIqKK2ksrr+/7EeKdH07xzHwC7xDOgSz4DO8d49Wd6YnDXw8ljbHOrJI/ljqaqAJwfa/5Or5/judQNJVTm8fgHsWgvXzIXUYc3b35q34ONbmn1RsC/pSBLNkbUKvn4M0ufZrryjb4VTnCNNY+zwJvu22MSydwvs+9HeF+85vI/gMHuEdESNq5ftPZa/09aQMpxzSLXbxXSwNaM0JyGl9PL/l600P7B6+dV26Jh3P0S3g0tnQrdRzdplTY1h5/4SNu8uZPPuIjbvKmJLbhEZecU09O8SEx5CfGQoCVH2Fh8ZSnxk2KHHCZG1y8OIjwwlNSmSuIgAm7Jh1uWwczHc8YPvalG1P6bTPrC13raqeJ/tzFBVZjszNHBh8jFlfAv/vsgeyF452//DKrlJE9TxyFxhE9PW+bbX1+hbnSaQRi/VOqz0wJFJqzZx5e+w3bzriuviJCSnlhSoXZ+P6uU303ZR97UjkuUkuOiFJifLA8UVbNpdyBYnEW3OLeLH3UWHrrcRgbTkaHp3iKVPp1i6JUWRGBVGvEviiYsMJdSNbtcBL2ctvHwmjL0Pxt7j/dczBp4fba+/u/m7wPyu+9KeTfDqBEjoDtf+D8JjmrZ93jZ4Zbw9ULvuC9s830I0K0GJyAfYDgz/dTo0+I1PElTmclj4GGz70p7nGXMbjLzec1fvV5bB/m02WeVtsz/yaafZZsCW9E/qzya/rFUw+xpbe3XjtWuvddmyu8jWinYXsXlXIXuKyg+VSYwKpU/HOPp0iqVPx1j6dIzjpA4xRIW1oUFc355qj8LvWOf9H7j0L+CtS+GiF2HwFd59rZYifT7M+oU94LrsTffPKZcegFfOttc8Xv+lPf3QgjQ3QZ0N/AoYBbwPvGaM2ezxKN3g1QS1c6lNTNsX2B5gY26DEdc3/UimLfF1k58xsPR5+OKPENsRc+lMDiQNZk9RGbmF5ewpLGNPkb3PLSw/tDy3sOzQaAFhwUGc2D7miETUp2Ms7WLDA2YMMr/Z9b3tCDN2Ooy917uv9dr59gDt9u/tmHXKWvYy/PduGPNbmPCnY5evrrTjF+5YYs/jdR/j/Rg9rFndzI0x84H5IhIPXAF8ISKZwAzgTWNMpUej9bUdi21i+ulr2xHhnEdg+HWamNxRby+/5jf5GWM4UFJJbmEZuU7SKcjL5bSNf6RvwbcsDxvFg2W3sP3FPCqqvzhq+9iIEDrERdA+NpyRPZLonBBxKBGlpUS3jiY5b+g0CPqcD0ueh1Nu8l4tKmeNHan8nD9pcqrrlBvsOe3FT9vzzkMbGZ7UGPjsd/DTN7Ym2gKTU2PcbrsQkWRgGnAVsAZ4CzgNuBrbXbzlyfjWJqaMRbbddsKjMPxanUiuqUTglBvtyBTvX2MvDhx7r53rKK6z7ezhNFUYY8gvqWRPUfkRyae2xpNbVMYep+bj2vttiKTzTNgzdJADPB9+HYtTptAvLoKxThLqEBdB+7hwOsRG0C42nMgwP02v0BqceQ9s/tReODtuundeY/EzdtzHYVd7Z/8t3aS/2ctLPr0DknrYUwD1WfIsrP63HQC2FTaTutvE9yHQB3gD27y3y2XdyvqqZt7ikSa+nxbZ7uIZiyC6PZx2Bwz7VUCOUdWSVFXXkLs3l4i5t5O88/NDy6sJYn9QMrtJIrMqkeyaJHabJHYZe59jkimPSCElLvpQomkfG0GHuHDax4RxcuYbpK5+HBPbmaApr9nrdpR3vXOlPSq/Y53nh4U6sAOeHmKnnJjwqGf33ZqU5sOr50DxXvj1l7bTlKsAHwC2KZo7ksSzxpiv6lvhy+TkEe9fAxs+skf1E/9qrxXQxOSWquoadhWUkXWglKwDJc794ce7C8ucqQ9+SS85i1TZS1poPj3DC+gakk8n8jilJof4irWE1NSZ10aCwHQE0xnoAtIFpDOsXwTpn0PfnyEXPNuieia1aGOn21rUkufhrD94dt9LX3Bq3Td7dr+tTWQCTH0XZoyHWZfBr784fLCwax188Gs76v5FL7bo5NQYdxNUXxFZbYzJBxCRROzcTs97LzQvOWE8dB1lmxZCI/0dTUAxxpCdX8rO/Ucnn+wjEpAlAh1iI0hNjGREWiKpiVGkJkaSmhhF54QIOsZH1N8Dzhjb66gwx7llOzfn8Z5NtjdTZbG9fmzy47YXZVvvwOBLHQdA3wtg2Yt2pBJPdXwpPWCbpAZc6pv5tVq6pJ52mvjXL7AH11fOttdMzbrMJqsr3m7VB9juNvGtNcYMrrNsjTFmiNcia4AOFus5B8urWJeZz+qdB1izM581mfmHpjgAmw86xUXQxUk6NvkcftwpPtJ7o0wbY4cuAq01+cvu9fDiqXZ8vrPu98w+v/k7fPUnuOk7mwSVe2ovaB5ylZ02pgUMANsUzW3iCxIRMU42c2bL1a43LUhNjWH7voOs3plvk9HOA/yYW0RtheiEdtGM79OeQV0T6JkSTWpiFB3jI3w+zcEhIpqY/K3jAOh3ISx9EUbd0vxaVFU5LHvJtmJocmqaIVfaC/2/exKQFj0AbFO4m6A+B94TkRexM6TdBPzPa1GpZisoqWRtlk1Eq3fms3bngUMTsMVFhDC4WyIT+3dkaPdEBqcm+G8MOBXYzrzXDr675Dk7xmFzrHvXDuPVFgeF9YTxf7QTbrbv16IHgG0Kd5v4goAbgfGAAPOAV4ypnZ7Ud7SJ72jGGNL3HGRlxgHW7DzAmsx8tu6xA9iKQO8OsQzplsiQbgkM7ZZAz5QYgoL0fI5y03tX2+G+7vjh+GtRNTXw/Cl2PrIbF+n5RHWE5l6oWwO84NxUAKiuMazeeYDP1+9m3sbcQ/MHJUWHMaRrAhcN7szQbomc3DWBmPA2NFSP8ryxTi1q8TNw9h+Pbx/p82wT1c+9MDGfarXc+uUSkZOAvwL9gEMzpRljWtaATy1ceVU1i7fm8fmG3czflMu+gxWEBQcx5sRkbjrzBMackEz35Cgdrkd5Vvu+0P9iO1rI6FshOrnp+1j8NMSlQv+LPB+farXcPbT+F/BH4J/AOOy4fPor6AOFZZUs3LKXzzfsZuHmPRRXVBMTHsLY3u2Y2L8jY3u3IzbQpm1Qrc+Z99jrB5c8YwfnbYqsVXZes4l/aTHTP6jA4G6CijTGfOn05NsBPCQii7BJS3nYnqIy5m/cw+cbdrN42z4qqw0pMWFcMLgzE/p3ZMwJyYSH6FA+yofa97GzvS6rrUU1YabixU9DeDwM/aX34lOtkrsJqszpKJHuTOOeDbT3XlhtT8a+YuZt3M3nG3JZvfMAxkC3pCiuGZPGxP4dGdItkWDt2KD86cx7YP2H9lzUOQ+7t83+n2DTHDsyt6emq1FthrsJ6g4gCvgt8CdsM5+O8thMxeVVvLY4gzlrc9iSWwRA/85x3DG+FxMHdKB3h1g9n6QCR7veMOASWD7DdhV3pxa15DmQYDsyulJNdMwE5VyUO8UYczdwEHv+STVDTY1h9uos/v75FvYUlTOyRxIPnN+PCf060DWp9Q5bolqBM++BDR/aZrtzHmm8bMl+WPMmnHwZxHXyTXyqVTlmgjLGVIvIMNeRJNTxW7Itj0c/28iGnEKGdEvghWnDGNbdw6NFK+Ut7XrZcfSWz4DRt0FMu4bLrngFqkphzK2+i0+1Ku6OY7MG+FhErhKRn9fejrWRiEwSkS0islVEjpqeU0S6icgCEVkjIutE5NymvoGWImNfMTf8eyVXzFhKfkklT10+mA9vHqPJSbU8Z/4/qCqDxU81XKay1A5rdNIE201dqePg7jmoJCAPOMtlmQE+bGgDp2nwOeAcIAtYISJzjDEbXYrdD7xnjHlBRPoBc4E098MPfAWllTzzZTqvL8kgLDiIuyf25rrTehARqr3wVAuVchIM/AUsfwXG3F5/Ler7d6Bkn+0codRxcnckieM57zQS2GqM2Q4gIu8AFwKuCcoAcc7jeCDnOF4nIFVW1zBr2U6enP8j+aWVTBnWld9P7EX72Ihjb6xUoDvj/8EP79vBSyf++ch1NTV2ptfOQxqeCVYpN7g7ksS/sMnkCMaYaxvZrAuQ6fI8CzilTpmHgHkichsQDZztTjyBzBjDwi17efSzjWzbW8zonsncf35f+neO93doSnlOyokwcAqseNXWkmI7HF63ZS7kbYVLZ+qwRm6orKwkKyuLsrKyYxdu4SIiIkhNTSU01L0Ltt1t4vvU9TWAizl2bae+b2bdJHcFdgr5f4jIaOANERngjP13eEciNwA3AHTr1s3NkH1vy+4iHv1sI4vS99EjJZoZvxzO2X3ba1dx1Tqd+f/gh/dsjz7XWtTiZyChG/S90H+xtSBZWVnExsaSlpbWqn8rjDHk5eWRlZVFjx493NrG3Sa+D1yfi8jbwPxjbJYFdHV5nsrRSe06YJLzGktEJAJIAfbUef2XgZfBjmbuTsy+tO9gOU988SPvLN9JTHgID5zfj6tGdfffXEpK+ULyCbYLuWstKnM5ZC6Fyf8HwTpIsTvKyspafXICEBGSk5PZu3ev29sc7y/oScCxqjIrgJNEpIeIhAGXA3PqlNmJncIDEemLrZ25H72flVVW88LCbYx9fCHvrsjkl6PT+PrucVx3Wg9NTqptOONuqK5wJtIDvnsKIhJg8JX+jauFae3JqVZT36e756CKOLJ5bjdwT2PbGGOqnGGRPgeCgZnGmA0i8giw0hgzB/g9MENE7nT2f01LudZq4ZY9PPDxejL3lzK+T3umn9uXE9vH+DsspXwr+QQYdDmsnAl9zofNn8Hpv4dw/V9QzeduE99xDaJljJmL7TruuuxBl8cbgVOPZ9/+lLm/hJveXEVqYhRvXncKp53UhIEzlWptzrjLdiufdZkdrfyUG/0dkWqC/Px8Zs2axS233NKk7c4991xmzZpFQkKClyJzs4lPRC4WkXiX5wki0mYndnn4kw0EifDva0dqclIqqScMugIqimxtKkbHkW5J8vPzef75549aXl3d+ITpc+fO9WpyAvfPQf3RGFNQ+8QYk08bnWrji425zN+0h9vHn0TnhEh/h6NUYBh7D/Q4E077nb8jUU107733sm3bNgYPHsyIESMYN24cU6dOZeDAgQBcdNFFDBs2jP79+/Pyyy8f2i4tLY19+/aRkZFB3759uf766+nfvz8TJkygtLTUI7G5282mvkTW5rrolFRU8dCcDfTqEMO1p7nXTS6sYGcAACAASURBVFKpNiGhG1xdtw+UaqqHP9nAxpxCj+6zX+c4/viz/g2uf+yxx1i/fj1r165l4cKFnHfeeaxfv/5QV/CZM2eSlJREaWkpI0aM4JJLLiE5+chZldPT03n77beZMWMGU6ZM4YMPPmDatGnNjt3dGtRKEXlCRE4QkZ4i8k9gVbNfvYV55qutZOeX8uhFAwkN1l56SqnWZ+TIkUdcp/T0008zaNAgRo0aRWZmJunp6Udt06NHDwYPHgzAsGHDyMjI8Egs7taCbgMeAN51ns/DjqPXZqTnFjHjm+1cOiyVkT2S/B2OUqoVaqym4yvR0dGHHi9cuJD58+ezZMkSoqKiGDt2bL0jXoSHhx96HBwc7NsmPmNMMXDUaORthTGG+/+znujwEKZP7uPvcJRSymNiY2MpKiqqd11BQQGJiYlERUWxefNmli5d6tPY3L0O6gvgF07nCEQkEXjHGDPRm8EFio/WZLPsp/385eKBJMeEH3sDpZRqIZKTkzn11FMZMGAAkZGRdOhweFzFSZMm8eKLL3LyySfTu3dvRo0a5dPYxJ3rYkVkjTFmyLGW+cLw4cPNypUrffZ6BSWVjH9iIamJUXx48xiCgtrGFd9KKd/YtGkTffu2nTmz6nu/IrLKGDO8bll3z/TXiMihoY1EJI16RjdvjR6ft5n9xRU8etEATU5KKeVD7naS+APwrYh87Tw/A2d08dbs+8x83lq2k6tHpzGgi06XoZRSvuRuJ4n/ichwbFJaC3wMeKabRoCqrrEdI9rFhPP7Cb38HY5SSrU57naS+DVwO3bKjLXAKGAJR04B36q8tWwHP2QX8MwVQ4iNcG9yLaWUUp7j7jmo24ERwA5jzDhgCC1oWoym2lNUxuP/28JpJ6Zw/smd/B2OUkq1Se4mqDJjTBmAiIQbYzYDvb0Xln/95bNNlFfV8MiF/dvMPC1KKRVo3E1QWSKSAPwH+EJEPubYU763SIu37uM/a3O46cye9Gync9oopVq3hkYzd8eTTz5JSUmJhyM6zK0EZYy52BiTb4x5CDvk0atAq5tuo6Kqhvs/Xk+3pChuGXeiv8NRSimvC+QE1eQRyY0xXx+7VMs0Y9F2tu8t5l+/GkFEaLC/w1FKKa9znW7jnHPOoX379rz33nuUl5dz8cUX8/DDD1NcXMyUKVPIysqiurqaBx54gNzcXHJychg3bhwpKSksWLDA47G1uSkzGpK5v4Snv0xn8oCOjOutE64ppfzgv/fC7h88u8+OA2HyYw2udp1uY968ecyePZvly5djjOGCCy7gm2++Ye/evXTu3JnPPvsMsGP0xcfH88QTT7BgwQJSUrwzcavOGeF4+JMNBAcJD5zfz9+hKKWUX8ybN4958+YxZMgQhg4dyubNm0lPT2fgwIHMnz+fe+65h0WLFhEf75uBC7QGBczbsJv5m/bwh3P76iy5Sin/aaSm4wvGGKZPn86NN9541LpVq1Yxd+5cpk+fzoQJE3jwwQe9Hk+br0GVVFTx8Ccb6d0hlmtOTfN3OEop5VOu021MnDiRmTNncvDgQQCys7PZs2cPOTk5REVFMW3aNO666y5Wr1591Lbe0OZrUE9/aWfJff+m0TpLrlKqzXGdbmPy5MlMnTqV0aNHAxATE8Obb77J1q1bufvuuwkKCiI0NJQXXngBgBtuuIHJkyfTqVMnr3SScGu6jePeucgk4CkgGHjFGPNYnfX/BMY5T6OA9saYhMb26cnpNtJzi5j81CIuHtKFx38xyCP7VEqpptDpNhqebsNrNSgRCQaeA84BsoAVIjLHGLOxtowx5k6X8rdhh1DyidpZcmMiQph+btv5ciilVEvhzTatkcBWY8x2Y0wF8A5wYSPlrwDe9mI8R6idJfeeSX1Iig7z1csqpZRykzcTVBcg0+V5lrPsKCLSHegBfOXFeA4pKKnkz59tYki3BC4b3tUXL6mUUg3y5qmWQNLU9+nNBFXfKKsNRXc5MNsYU13vjkRuEJGVIrJy797mD6L++LzNHCjRWXKVUv4XERFBXl5eq09Sxhjy8vKIiIhwextv9uLLAlyrJ6k0PMDs5cBvGtqRMeZl4GWwnSSaE9RaZ5bcX43pQf/OOkuuUsq/UlNTycrKwhMH34EuIiKC1NRUt8t7M0GtAE4SkR5ANjYJTa1bSER6A4nYCRC97i9zN9E+Npw7zznJFy+nlFKNCg0NpUePHv4OIyB5LUEZY6pE5Fbgc2w385nGmA0i8giw0hgzxyl6BfCO8VH99p+XDSb7QKnOkquUUgHOq9dBeYMnr4NSSinlfw1dB6VDJyillApILa4GJSJ7gR3N3E0KsM8D4fhSS4u5pcULGrMvtLR4QWP2he7GmHZ1F7a4BOUJIrKyvupkIGtpMbe0eEFj9oWWFi9ozP6kTXxKKaUCkiYopZRSAamtJqiX/R3AcWhpMbe0eEFj9oWWFi9ozH7TJs9BKaWUCnxttQallFIqwGmCUkopFZBadYISkUkiskVEtorIvfWsDxeRd531y0QkzfdRHoqlq4gsEJFNIrJBRG6vp8xYESkQkbXO7UF/xFonpgwR+cGJ56ghPsR62vmM14nIUH/E6RJPb5fPb62IFIrIHXXK+P1zFpGZIrJHRNa7LEsSkS9EJN25T2xg26udMukicrUf431cRDY7f/ePRKTe2bKP9R3yccwPiUi2y9/+3Aa2bfS3xccxv+sSb4aIrG1gW798zs1ijGmVN+z4f9uAnkAY8D3Qr06ZW4AXnceXA+/6Md5OwFDncSzwYz3xjgU+9fdnWyemDCClkfXnAv/FTr8yCljm75jrfEd2Yy8SDKjPGTgDGAqsd1n2f8C9zuN7gb/Vs10SsN25T3QeJ/op3glAiPP4b/XF6853yMcxPwTc5cb3ptHfFl/GXGf9P4AHA+lzbs6tNdeg3JnR90LgdefxbGC8iPhlgihjzC5jzGrncRGwiQYmeGxhLgT+baylQIKIdPJ3UI7xwDZjTHNHJvE4Y8w3wP46i12/r68DF9Wz6UTgC2PMfmPMAeALYJLXAnXUF68xZp4xpsp5uhQ75U7AaOAzdkdTZwv3mMZidn67puDDmcm9rTUnKHdm9D1UxvlHKgCSfRJdI5ymxiHAsnpWjxaR70XkvyLS36eB1c8A80RklYjcUM96t2dW9oPLafifOdA+Z4AOxphdYA9ogPb1lAnUz/tabE26Psf6DvnarU6z5MwGmlED9TM+Hcg1xqQ3sD7QPudjas0Jyp0ZfZsy669PiEgM8AFwhzGmsM7q1djmqEHAM8B/fB1fPU41xgwFJgO/EZEz6qwPuM8YQETCgAuA9+tZHYifs7sC7vMWkT8AVcBbDRQ51nfIl14ATgAGA7uwTWZ1Bdxn7LiCxmtPgfQ5u6U1Jyh3ZvQ9VEZEQoB4jq/K7xEiEopNTm8ZYz6su94YU2iMOeg8nguEikiKj8OsG1OOc78H+Ajb/OGqKTMr+9JkYLUxJrfuikD8nB25tc2jzv2eesoE1OftdNI4H7jSOCdC6nLjO+QzxphcY0y1MaYGmNFALAH1GcOh36+fA+82VCaQPmd3teYEdWhGX+do+XJgTp0yc4DaXk6XAl819E/kbU778avAJmPMEw2U6Vh7jkxERmL/fnm+i/KoeKJFJLb2Mfak+Po6xeYAv3R6840CCmqbqfyswaPNQPucXbh+X68GPq6nzOfABBFJdJqnJjjLfE5EJgH3ABcYY0oaKOPOd8hn6pwfvbiBWNz5bfG1s4HNxpis+lYG2ufsNn/30vDmDduD7Edsj5s/OMsewf7DAERgm3i2AsuBnn6M9TRsM8E6YK1zOxe4CbjJKXMrsAHba2gpMMbPn29PJ5bvnbhqP2PXmAV4zvkb/AAMD4DvRRQ24cS7LAuozxmbPHcBldgj9uuw50e/BNKd+ySn7HDgFZdtr3W+01uBX/kx3q3YczW13+faHrOdgbmNfYf8GPMbzvd0HTbpdKobs/P8qN8Wf8XsLH+t9vvrUjYgPufm3HSoI6WUUgGpNTfxKaWUasE0QSmllApImqCUUkoFJE1QSimlApImKKWUUgFJE5RSLZQz6vqn/o5DKW/RBKWUUiogaYJSystEZJqILHfm4XlJRIJF5KCI/ENEVovIlyLSzik7WESWusyhlOgsP1FE5jsD2K4WkROc3ceIyGxn3qW3/DUav1LeoAlKKS8Skb7AZdiBOgcD1cCVQDR2LMChwNfAH51N/g3cY4w5GTuiQe3yt4DnjB3Adgx2NAGwo97fAfTDjhZwqtfflFI+EuLvAJRq5cYDw4AVTuUmEjvIaw2HB/Z8E/hQROKBBGPM187y14H3nTHUuhhjPgIwxpQBOPtbbpzx15yZVNOAb73/tpTyPk1QSnmXAK8bY6YfsVDkgTrlGhtzrLFmu3KXx9Xo/7RqRbSJTynv+hK4VETaA4hIkoh0x/7vXeqUmQp8a4wpAA6IyOnO8quAr42dFyxLRC5y9hEuIlE+fRdK+YEebSnlRcaYjSJyP3Ym0yDsKNS/AYqB/iKyCjuT82XOJlcDLzoJaDvwK2f5VcBLIvKIs49f+PBtKOUXOpq5Un4gIgeNMTH+jkOpQKZNfEoppQKS1qCUUkoFJK1BKaWUCkiaoJRSSgUkTVBKKaUCkiYopZRSAUkTlFJKqYCkCUoppVRA0gSllFIqIGmCUkopFZA0QSmllApImqCUUkoFJE1QSimlApImKKV8TEReE5FH3SybISJnH6PMQyLypmeiUypwaIJSSikVkDRBKaWUCkiaoJRqgNO8dreIrBORYhF5VUQ6iMh/RaRIROaLSKJT9gIR2SAi+SKyUET6uuxniIisdrZ5F4io8zrni8haZ9vFInJyM+NuLJZ7RCTbiWWLiIx3lo8UkZUiUigiuSLyRHNiUMoTNEEp1bhLgHOAXsDPgP8C9wEp2P+f34pIL+Bt4A6gHTAX+EREwkQkDPgP8AaQBLzv7BMAERkKzARuBJKBl4A5IhJ+PMEeI5bewK3ACGNMLDARyHA2fQp4yhgTB5wAvHc8r6+UJ2mCUqpxzxhjco0x2cAiYJkxZo0xphz4CBgCXAZ8Zoz5whhTCfwdiATGAKOAUOBJY0ylMWY2sMJl/9cDLxljlhljqo0xrwPlznbHo7FYqoFwoJ+IhBpjMowx25ztKoETRSTFGHPQGLP0OF9fKY/RBKVU43JdHpfW8zwG6AzsqF1ojKkBMoEuzrpsc+TU1TtcHncHfu80x+WLSD7Q1dnueDQYizFmK7Zm9RCwR0TeEZHa17kOW0vcLCIrROT843x9pTxGE5RSzZeDTTQAiIhgk0w2sAvo4iyr1c3lcSbwZ2NMgsstyhjzthdiwRgzyxhzmlPGAH9zlqcbY64A2jvLZotI9HHGoJRHaIJSqvneA84TkfEiEgr8HttMtxhYAlRhz1WFiMjPgZEu284AbhKRU8SKFpHzRCTW07GISG8ROcs5v1WGrQFWA4jINBFp59S48p19VR9nDEp5hCYopZrJGLMFmAY8A+zDdqb4mTGmwhhTAfwcuAY4gD1H9KHLtiux56GeddZvdcp6PBbs+afHnOW7sbWl+5xNJwEbROQgtsPE5caYsuONQylPkCObxpVSSqnAoDUopZRSAUkTlFItgHNx8MF6bvcde2ulWiZt4lNKKRWQQvwdQFOlpKSYtLQ0f4ehlFLKQ1atWrXPGNOu7vIWl6DS0tJYuXKlv8NQSinlISKyo77lbe4cVHlVNVkHSvwdhlJKqWNocwnqutdWcv2/V1Fdo+felFIqkLW5BHXFyG5s2lXI28t3+jsUpZRSjWhx56Ca69yBHRnVM4m/z9vCeQM7kRgd5u+QlFJtWGVlJVlZWZSVtf6BOyIiIkhNTSU0NNSt8m0uQYkID13Qn3OfWsQTX/zIny4a4O+QlFJtWFZWFrGxsaSlpXHkmMKtizGGvLw8srKy6NGjh1vbtLkmPoA+HeO4alR33lq2g405hf4ORynVhpWVlZGcnNyqkxPYykFycnKTaoptMkEB3HlOL+IjQ3nokw3oxcpKKX9q7cmpVlPfZ5tNUAlRYdw9sQ/Lf9rPp+t2+TscpZRSdbTZBAVw2Yiu9O8cx1/mbqKkosrf4SillM/l5+fz/PPPN3m7c889l/z8/GMXbIY2naCCg4SHL+jProIynl+wzd/hKKWUzzWUoKqrG5+vcu7cuSQkJHgrLKCNJyiA4WlJXDS4My9/s50decX+DkcppXzq3nvvZdu2bQwePJgRI0Ywbtw4pk6dysCBAwG46KKLGDZsGP379+fll18+tF1aWhr79u0jIyODvn37cv3119O/f38mTJhAaWmpR2Jrc93M63Pv5L7M25jLo59tYsYvh/s7HKVUG/XwJxs83rO4X+c4/viz/g2uf+yxx1i/fj1r165l4cKFnHfeeaxfv/5QV/CZM2eSlJREaWkpI0aM4JJLLiE5OfmIfaSnp/P2228zY8YMpkyZwgcffMC0adOaHXubr0EBdIyP4LazTuKLjbl8/eNef4ejlFJ+M3LkyCOuU3r66acZNGgQo0aNIjMzk/T09KO26dGjB4MHDwZg2LBhZGRkeCQWrUE5rj0tjXdX7OThTzbwv9vPICxEc7dSyrcaq+n4SnR09KHHCxcuZP78+SxZsoSoqCjGjh1b73VM4eHhhx4HBwd7rIlPf4Ud4SHBPPizfmzfW8zrizP8HY5SSvlEbGwsRUVF9a4rKCggMTGRqKgoNm/ezNKlS30am9agXJzVpwPjerfjqS/TuXBIZ9rHRvg7JKWU8qrk5GROPfVUBgwYQGRkJB06dDi0btKkSbz44oucfPLJ9O7dm1GjRvk0thY35fvw4cONNycs3L73IBOf/IYLB3fh778Y5LXXUUopgE2bNtG3b19/h+Ez9b1fEVlljDmqh5pXm/hEZJKIbBGRrSJybwNlpojIRhHZICKzvBmPO3q2i+G603oye1UWq3ce8Hc4SinVZnktQYlIMPAcMBnoB1whIv3qlDkJmA6caozpD9zhrXia4tazTqR9bDgPzdlAjU5sqJRSfuHNGtRIYKsxZrsxpgJ4B7iwTpnrgeeMMQcAjDF7vBiP22LCQ5h+bh/WZRUwe1WWv8NRSqk2yZsJqguQ6fI8y1nmqhfQS0S+E5GlIjKpvh2JyA0islJEVu7d65vrlC4a3IWh3RL42/82U1Ba6ZPXVEopdZg3E1R946rXbS8LAU4CxgJXAK+IyFGDOxljXjbGDDfGDG/Xrp3HA62PiPDIhQPYX1LB018efWGaUkop7/JmgsoCuro8TwVy6inzsTGm0hjzE7AFm7ACwoAu8Vw+ohuvL84gPbf+6wSUUkp5hzcT1ArgJBHpISJhwOXAnDpl/gOMAxCRFGyT33YvxtRkd03oRVRYsE5sqJRqlY53ug2AJ598kpKSEg9HdJjXEpQxpgq4Ffgc2AS8Z4zZICKPiMgFTrHPgTwR2QgsAO42xuR5K6bjkRwTzu/O6cV3W/P4fEOuv8NRSimPCuQE5dWRJIwxc4G5dZY96PLYAL9zbgFr2qjuzFq+k0c/28jY3u2ICA32d0hKBZaiXJh7F0z8CyR0PXZ5FTBcp9s455xzaN++Pe+99x7l5eVcfPHFPPzwwxQXFzNlyhSysrKorq7mgQceIDc3l5ycHMaNG0dKSgoLFizweGw61JEbQoKDeOiC/kydsYyXv9nOb8cHzGkypQLD0udh0xwIi4aLX/R3NC3Xf++F3T94dp8dB8Lkxxpc7Trdxrx585g9ezbLly/HGMMFF1zAN998w969e+ncuTOfffYZYMfoi4+P54knnmDBggWkpKR4NmaHDhbrpjEnpHDewE48v3ArWQe8V6VVqsWpKIZVr0FIBKx7F/Zu8XdE6jjNmzePefPmMWTIEIYOHcrmzZtJT09n4MCBzJ8/n3vuuYdFixYRHx/vk3i0BtUE08/tw5ebc/nr3M08d+VQf4ejVGBY9y6U5cNlb8JHN8HCv8IvXvN3VC1TIzUdXzDGMH36dG688caj1q1atYq5c+cyffp0JkyYwIMPPljPHjxLa1BNkJoYxc1nnshnP+xi8bZ9/g5HKf+rqYGlL0CnQdDnfBh1M2z4yPPNVMprXKfbmDhxIjNnzuTgwYMAZGdns2fPHnJycoiKimLatGncddddrF69+qhtvUETVBPdeGZPuiRE8vCcjVRV1/g7HKX8a/tXsO9HGHULiMDoWyEiHr76s78jU25ynW7jiy++YOrUqYwePZqBAwdy6aWXUlRUxA8//MDIkSMZPHgwf/7zn7n//vsBuOGGG5g8eTLjxo3zSmw63cZx+N/6Xdz05mquGZPGrWedSEpM+LE3Uqo1evNS2PU93LkeQpz/g28eh68ehV9/CalHzaCg6tDpNpo53YaI3C4icWK9KiKrRWSCh+JtcSb278iFgzvz2uIMRv3lS256YxULtuyhWkc+V23J3h9h6xcw4teHkxPAKTdDVLJNUko1g7tNfNcaYwqBCUA74FeAf8/m+ZGI8NTlQ/jizjO4ZkwayzP286t/reC0v33FP+ZtIXO/9vJTbcDylyA4DIZfe+Ty8Bg47XewfQFkfOuf2FSr4G6Cqh349VzgX8aY76l/MNg25aQOsdx/fj+WTh/P81cOpVeHWJ5dsJXT/28BV76ylI/XZlNWWe3vMJXyvNIDsHYWDPwFxNQzgPOI6yCmo61FtbDTCP7Q0k61HK+mvk93u5mvEpF5QA9guojEAtpDwBEWEsS5Aztx7sBOZOeXMntlFu+tzOT2d9YSHxnKxUO6cNmIrvTtFOfvUJXyjNVvQGUJnHJT/etDI+GMu+zoEtu+hBPP9m18LUhERAR5eXkkJycj0nqP+40x5OXlERER4fY2bnWSEJEgYDCw3RiTLyJJQKoxZt1xR3ucAqGThDtqagzfbdvHuysymbchl4rqGk5OjWfK8K5cMLgzcRGh/g5RqeNTXQVPD4aE7vCrzxouV1UBzwyD6GS4foHt5aeOUllZSVZWFmVlZf4OxesiIiJITU0lNPTI37+GOkm4m6BOBdYaY4pFZBowFHjKGLPDQ3G7raUkKFcHiiv4aE02767IZEtuERGhtsZ1+YhujEhLbNVHTaoV2vAfeP9quOwt6Ht+42XXvAkf/wYunwV9zvNNfKrFaW6CWgcMAk4G3gBeBX5ujDnT04EeS0tMULWMMXyfVcC7KzL55PscDpZX0TMlmokDOjKqZzIj0hKJCtPBPVSAmzkJCnPgt2sg6BgDJ1dXwXMj7TBIN30LQXrppTpaQwnK3V/DKmOMEZELsTWnV0Xkas+G2PqJCIO7JjC4awIPnN+Xz9btYvaqLGZ8s50XFm4jJEgY1DWBUT2TGN0zhWHdE4kM05HTVQDJWQM7l9hRy4+VnACCQ2DcffDBdbDhQxh4qfdjVK2GuzWor4H/AdcCpwN7sU1+A70b3tGaXYNa/CwU74XxD7r3D+YDxeVVrNxxgCXb8li6PY8fsguorjGEBgtDuiYyqmcSo05IZmi3RJ3qQ/nXhzfC5k/hdxvtiBHuqKmBF0+F6gq4ZZlNWkq5aG4N6jJgKvZ6qN0i0g143JMB+syBDFgxw44VdumrEJno74iIDg/hzF7tOLOX7a5bVFbJyowDLN2ex5LteTy7YCtPf7WVsJAghnRNYPQJyYzqmcyQbgmEh2jCUj5StBvWf2Cve3I3OYFt1ht3H7w7zQ4sO+RK78WoWhW3hzoSkQ7ACOfpcmPMHq9F1QiPnINa+S+Ye7edWO3yWdA+sIcZKSyrZMVP+20N66c8NuQUYgyEhwQxrHsio3omM/qEZPp3jtNzWMp7vvqzHcbotlWQfELTtjUGXh4Lpfvh1lUQEuaVEFXL1NxOElOwNaaF2At0T8dOzz7bw3Eek8c6SexcCu9eZa/luPilY/dGCiAFJZUs+ymPpdv3s2R7Hpt2FQK2F2+3pCh6d4ilT8dYeneMo3fHWNKSowgJ1pPTqhkqy+Cf/e3YelPfPb59pM+Hty6B8/5hh0dSytHcBPU9cE5trUlE2gHzjTGDPB7pMXi0F19hDrxzJeSshjPvhTPvaZG9jA4UV7AiYz+bdhWxeXchW3YXkZFXTO3QgOEhQZzUIYbeHeKcxGUTWLvYcO3irtxT2138lx9Dz7HHtw9jbA/A/B22B2BopCcjVC1YcxPUD64dIpwLd78/VicJEZkEPAUEA68YY+odv09ELgXeB0YYYxrNPh7vZl5ZBp/eCd/Pgt7n2tpURMsf8aGsspr03IOHEtaW3CI27y5ib1H5oTKJUaFOsrI1rd4dY+nVIZaYcG0mVC6MgRdPB1MNNy9u3gW3Py2C18+3vQBH/8ZzMaoWrbmdJP4nIp8DbzvPLwPmHuMFg4HngHOALGCFiMwxxmysUy4W+C2wzM1YPCs0Ai563k649vl98MrZ9rxUyol+CcdTIkKDGZgaz8DUI09m7y+uOJy0dtuk9d7KTEoqDo8ZGBMeQnJMGMnRYSTHhJMSE0ZydDjJMWEkRYeREhPurA8nMSpUmw9bu4xvIfcH+NnTzR8Nosfptga26AkYerUdWFapBjSlk8QlwKnYc1DfGGM+Okb50cBDxpiJzvPpAMaYv9Yp9yQwH7gLuMvnNShXP30D710NNdVwySvQq23MKFJTY8g6UMrm3YWk7znIvoPl5B2sIK/Y3u87WMGBkop6pxMRgcQom8xck1eHuAhOTo1ncNcEYnVYp5btnSthx2LbtdwTzXKZK+DVs+2lHqf/vvn7Uy1ec2tQGGM+AD5owmt2ATJdnmcBp9QJagjQ1RjzqYjc1dCOROQG4AaAbt26NSGEJupxBtywEN69EmZNsf9Ap93Z6scQCwoSuiVH0S05ign96y9TU2MoKK0kr7icfQcrDiUw+7ic/cV22abdheQdrKCgtNLuW6B3xziGdU9gWPdEhnVLomtSpJ77ain2/wSbP4PTHG+zegAAGS5JREFUf+e5c0ZdR0CvSfDdUzD8OohM8Mx+VavTaIISkSKgviqWAMYY09jJmvp+gQ7tyzmP9U/gmmMFaYx5GXgZbA3qWOWbJbE7XDsP5twKXz5sZwu96HkIi/bqywa6oCAhMTqMxOgwTmx/7PKFZZWs3ZnPqh0HWL3zAP9Zk8ObS3cCkBITfjhhdU9iQJc4vZ4rUC1/2V7Q7uled+Pug5fOgCXPwVl/8Oy+VavRaIIyxsQ2Y99ZQFeX56lAjsvzWGAAsNA5mu4IzBGRC47VzOd1YVFwyav2vNT8hyBvK1z+FiSm+TWsliQuIpQzerXjDOfi4+oaw4+5RTZh7TjAyh0H+HxDLgBhwUEMTI1nWPdEhnZLZFj3RNrFhje2e+ULZYV2Wo3+F0NcZ8/uu9Mg6HchLH3eTtkRnezZ/atWwe1zUE3esUgI8CMwHsgGVgBTjTEbGii/EH+fg6rP1vkw+1qQIPjFa8ffxVYdZU9RGf+/vXOPj6q69vh3zUwmT0iISQgP5e0DFRDR67sqqMCtWq1Vsaj1Ua9Vq7a3H22rttZbb2utfmytV2sVi9WqFbFiC1YU3xVFEAFF3g8JgZCQ92syM/v+sU4yk2ESAsnMnIT9/XzO5+xz9j5n1uw5M7/Ze6+99rItVSzbWsnSLZWs3FZNIKTLjA07KItjDxnA+IPzGFWYw4jCbAb1z8DjsV2DSWPxY/Da7XDtIhh6bM/ff9ca+L8T4MSb4Oz/6fn7W3oN3XIz78aLTgceQt3MZxlj7hWRe4BPjDHzYsq+jRsFCqBigw4Ul6+Bs38JJ9zQ58elukXZlxqv7eDj4ZATwds1J4nmYIhVJTUs3bKbpVsqWbqlivK6iFt8us/DiIJshh+UzYjCbEYUZDOyQPf52X47rtWThMPw8ETILoRrFybudeb+F3zxCtyyHPoVJ+51+gLNdfDKDVA0Vuds9qHnPSUClQhSttxGcy28fL3+8I67FM59yE40jKVuF7z9v7B0ts6ZAY3ZNvosOGwajJ68T7EPjTHsrGlmY3kdm8rr2Vxez6byejaW17O1ooFglFdhvwxfm1iNKMhheEEWI5299SLcD9YsgOcuhYtmwVHfTNzr7N4ID0/SJeKn987wnkkhUA/PXgxb3tfjM+6Ar92WWpt6ECtQPUE4DO/9Ft66FwZNgGn36aqiOUWuiYyeElqa4KNH4d0HNHTUcdfASd+H0hWwdgGs/ZdGkBcvDDtJxerQqfsezy2KYCjMtspGNlXUs2mXClfrtr26kejHurBfOsPysxgyIJPBeboNyctoS9vVjeMw+1ztObjlsy63gPebeTfD8r/CzcsgL4Feur2VQIN6FW/5AC54HDYs0sACU38NJ3wv1db1CFagepIv58Pc6yBQq8ceH/QbBP2HQO4Q3Uenc4dCVkGvDKPUKcZodOs3fgHVW+HQaXDWPVB4aPty4TCULIU182Hta1DmzNUuOAwOm6oRPIYe12Mi39QSYktFA5vK69hYrgK2ZXcDpdWNlFY1tWt5gba+huS1ileGI2ARMRvYL/3Amoy883N49CSYcrdOs0g0VV9pd+L4S+G8hxP/er2JlkZtyW56V6PcjLtYF4Gc8x1Y/Sqc9weYeHmqrew2VqB6mtodunhb9TaN6VdTAtUlUOMchwLty3v9KmK5Qx0BGxxJ5x2sLbHeFGLpq4818sa2JTDwaDjnl113IKncDGte09bV5vchHISsg2DM2dq6GnUmpHfHgbRjQmFDeV0zJVWNbHe2kspGSqqa9Li6kaqGlnbXeD1Ccf8MBudlkJ/tJ8vvI9PvJTPNS5bf2y6dkebV/DQ9nxWd5/eSleZ1v9i9chOsnKMTc7Pyk/OaC26Hj/8ENy3pVsu6T9HSBM/PgA1vwQWPqYC3EmxW4dr4tnbDHnlByszsCaxAJZNwGBoqVKyqS1S82gTM2ddu1x/maDLzdR5W3rCY/XAVMZ8LXK8rN2uL6fO5kFMMk++C8TP2v/XTVA3r39Qxj3WvQ1OVivnwU7Rlddg0FfIkUt8cpLQ6SrSqGilxhKyqoYWGliCNgRCNgRANLSH29Svk93oo7JfOqKIcRhZkM6ooh1EF2YwszGFg/xQH8K0vhwfHwoTLdJw1WdTuhN+NhyPOhW/+KXmv61ZamjRgwPo34fxH4q+hFaiHv1yovRMznocxU5JvZw9hBcpthMNQX6ZiVbVFt8ro/VYIR/+TF22BxRWwYdoiS+Q4WFM1vPcALH5Ux5JOvkXHmXoyllooCF8tVrFaswB2bwBEW1bHXQujp7ium9QYQ3Mw3CZWjYEgjYEwDYEgjS2OiAVC7dINLUFKq5rYsKuOjbvqaWyJxEHM9nsZWZjDqMJsZ5/DSMdjMSmrKb97Pyz6pa58W3R44l8vmoU/gw9+Dzd86Po12hJKsFkXd1z3unZ5Tryi47JN1fDnr0P5Opj5Egw/OXl29iBWoHob4RDUlsaIVtS+Zjvtgnx40qDwcBg0TidBFo+D4qO631UWCsLSp+DtX0HDbm0tTb6r5yduxqN8Haz4GyybDXU7VYiPuwYmzOwzEzuNMeyoaWJDWT0by1WwWoWrpKqxrZwIDMnLbC9eBdkcnJ9FUf/0nonEEQzA78apOFzeaajNxNCwGx4aB6POgEv+kvzXdwPBAPztCu3+/vpDMOmqvV9TXw5PTYOaUrhyHgyZmHg7exgrUH2NYLOOf1VuVsHavQl2rlLPuYZyp5Bof37xOBWuYke8sgv2fn9jYN1CeP1Onf81/FSdAzZ4QiLfVXyCAXXvX/Kkutl60+GoC7VVNeTYPjUfJJqGQFDd6nfVR4TLEbHo6PMABTl+inMzKO6fyaDcDCedEUnnZux9teUVL8Lca+Hbc2DMWQl8Z53w1q/gnV/DmXfCqT/qs59tXEItGqx6zT/3fVHHmu0w6xydK3XVguS3fruJFagDBWO05VW6Anas0FiCpSvUy66V/kP2FK3coZEfgx2rVJg2vgX5o3SW/2HT3fFjUbYaljwBnz0PgTp19z/uWp2r489KtXVJobXV1drK2lndRGlNEzuqmyitbmJHdSOVMY4eALmZae3EqzhXBWxg/wwyfB6OXnAB3kAd6761CK/Xi88reETweQSvR/B5de8Vwefx4PVG8rwiPRPlo6VJ42CufBHGfuPAiYMZaoE5V6ln3vTfwvHf3fd77N4Is6Zp+urXIH9Ez9qYQKxAHeg07IYdK9uLVsU6MBpaiMwBKlYZudpaSe8Pp/8EJl0NPn9qbY9Hcy2seAE+fgJ2rVa7J8zULkDrBUZTSygiWDWNlFY3qZBVN7GjRvfldc1tDh4TZS1z0+/mzpareCa0f60nj0D/zDTyMtPIzfKT66TzsnTfPzONvCy/k5/Wts/NTGvfRWkM/Pv3sPDnMPAomPHXvj0/KhSEl66BL/4OU++DE67f/3uVrdbuvvT+KlLJ6IrvAaxAWfYk0KBzXnZ8FhGtqi0w/jI47UfJczHuDsbA1g/VRXn1PPWMHHWmtqrGnANeuzpwRwSCYcpqm9hZ08SwRTeSt/09/n3eewS8WQTDhlDYEAyHCTlpPY7sw23HYYJhQ0soTE1jkOrGFqoaW6huCETSjS2dejtmpnnJc8RKBc3P8aFlXPbV3RiPj39PfJCWg08iN9Ov+VlpDMjyJ8dxZB8xxmAMhIwh7KTDxpDu8+KNbWWGgjD3u+oV21OrDJcshdnnqzhdNb9rXfopxgqUpe9TuxOWPa1OHTUl0H8oTPqOrtya04U1Qg5Uqrepc8KJN+g4YwIIhw21Ta3iFaCqoWVPIWtoPY6UyWvYwqPe+zlEyvhF8Io9WnfpPk+boLUKV2s6y+8jGA4TCIUJhlRAW0JhAkFNB8ORdFteyBBsOzYEgloubFR4wkbn0rUKT3Q6bAwhJx0Pv8/DyIJsxgzsx+jCHMYUZXLKyjvov+7vOsH95Ft6rsI3fwDPXAiFh8GVr2oPg4uxAmU5cAgF1QtqyRM6kdGTpnEAix0PtYFH6tiabV0pC3+uXWq3fOa6rjRjDI21lXhe/i4Zm95gx5gZfHrUT6hsEqoaA1Q3tFDZEGgnbq3HgVAYEZ13lub1kOYVZ+/B72t/3D7twe+LHLeOr3k94BEdlxOh7byInveK4BEQp4zXE0l7BCrqA6wvq2NdWS3bK+u53/cYF3rf5zfBS3ltwAzGFOUwuiiHMUX9GF2kUwwy/d1oIa59XSf6Dj0OZs519RitFSjLgUn5OvX+W/+GzqtqHXPz+jXUUtERMHCsRoguGtveWcTNVG3VKBwNu3W+XCjo7Ft0Hw5F0u3ygrpF523/VAXcza7d4ZDOz3r/QY2Qf/HTnbaKW1s7e3SpuYFwiODLN+Jb+Ryrj7iZf+bNbBOuLTFBkIcOyHRES8VrdFE/hg7IJD/bT1pXIpKsmqvjWyPPgBnPuWOyfxysQFksLU3qMl+2WsfeylZrXMCakkiZ9P4qWkVHQNGRkRZXqsfjmmpg83sa9mbDImcScxw8Pm0xetM07U3TY483kvbGlPFlqKfmwCOT+572h5VzNBRT1kG6iGgqpj10h3AYXv0+fPoMnP5TOP32dtmBYJgtFfWOYNWx3tk27KqjORhuVzYvK42CnHQOyvZTkJNOQY6fg3LS9VxO5NzADXPImH8zHHEeXPSUK3sOrEBZLB3RWBURq7IvIgLWVBUpk1PsiNZYnWNSeIT27ycqfmKoRQe7N7yl7v7bPtElTNKyNAzUyDM09mHu0PYi1Btaf91l+3Jdn62hAs7/Axx9Uaot6hrhMPzjVp14/rXbddn7LhIKG0oqG1lXVktpdRMVdQHK65qpqG+mvDZAeX0zFXU6lheP6/yv8VPP07yZPoXnim8jv18G/TPSyMnwkZPuIztd93umveRk+HpmIngnWIGyWPYFYzQgcKxo7VoDwUiEB3IP1ggeraJVdLge7+vcHWN0eYsNi1SQNr3nRMsXGHyMRlcYdSYMPd6dbv/Jpm4X/O1y9eA85Qdw5l3uXvLGGPjnD+GTWXDqf6u9CfgzEQiG2V2v4qVbgAonfeymx5la/hSvpJ/LvaErqW0OtQuz1RlpXmknXLn+MMW+Bgp9jUw9YTyTxo7ult1WoCyWniAcUlf8si91/lXZak2Xr4VQZPVf8g7R1lbh4dryKjxcW1zRi1zWV8Cmt51W0ttQ/ZVz7TAVpJFnwIjTUt+96FaCAVhwm3ptjjkbvvlEz3qrhVr0z0nVV/rZBgNR+0D7c6EWje7S0bnGSp2HePKtuoxJKlq6xugE/A//AKfdBmfeQSjQSEP1LppqymmuKSdQW0GovpJQw25o3I2nsQpPcxVpgSrSA9WkB6vJCtWSbprabrvyuPs4+j+7MXcLK1AWS2IJBTXsVJtorYZdX6qTRlvQX4EBw1WsarfrvDMMpOfCiFO1hTTqDMgfmbr30RtZ8qQK1YAR6ghQMGbf7xEOQcV6KFkG25ep48iOlRBs2vu1njR1PvCmaRgun1/3Xn8k7fPDqMnqSp7Kblhj4NWbdTqGL7N9b0AsnjT9c5Q5wNla03mRc1n5MGSSrrbQDaxAWSypINSiIWjaRGu1dhNm5kdaSYOPceXAda9i8wfa5RcKwkVPdh5L0Bj9M9EqRCWfQulyDZ0FkJatzheDj9GtYIw6knj9juikR9Jev+si7O+VcAg+fERXU9hDfKKEJy0raWKaEoESkanA7wAv8IQx5tcx+T8ErgWCwC7gamPMls7uaQXKYrHEpWorPHeZBk2ecnektVKz3RGiqNZRY6Ve4/VD8dEweKJGAR98DBQc6u7xrD5I0gVKRLzAWuAsYBuwBJhhjPkiqswZwEfGmAYR+R5wujHmks7uawXKYrF0SKAeXrkRPn9ZAwnX7oC6HZonXh0XHDzBEaOJemydTlJORwKVyH6F44H1xpiNjgHPA+cDbQJljHkrqvxiYGYC7bFYLH0df7bO9Rk0Hla9pE4mrWJUfLSroylY9iSRAjUE+CrqeBvwH52UvwZYEC9DRK4DrgM45BB3hWKxWCwuQ0Rdz0/5QaotsXSTRI7uxRtdi9ufKCIzgUnA/fHyjTGPG2MmGWMmFRYW9qCJFovFYnEriWxBbQOifQ+HAttjC4nIFOAO4GvGmObYfIvFYrEcmCTSScKHOklMBkpQJ4nLjDGfR5U5BpgDTDXGrOvifXcBnXr6dYECoHyvpdxFb7O5t9kL1uZk0NvsBWtzMhhmjNmjeyzRbubTgYdQN/NZxph7ReQe4BNjzDwReQM4Gih1LtlqjDkvYQZF7PoknseIm+ltNvc2e8HanAx6m71gbU4lCZ0daIyZD8yPOfezqPSURL6+xWKxWHovvWwKtMVisVgOFA5UgXo81QbsB73N5t5mL1ibk0FvsxeszSmj18Xis1gsFsuBwYHagrJYLBaLy7ECZbFYLBZX0qcFSkSmisgaEVkvIj+Ok58uIi84+R+JyPDkW9lmy8Ei8paIrBaRz0XkljhlTheRahFZ7mw/i3evZCIim0VkpWPPHlF8Rfm9U8crRGRiKuyMsuewqPpbLiI1InJrTJmU17OIzBKRMhFZFXUuX0QWisg6Zz+gg2uvdMqsE5ErU2jv/SLypfO5vywieR1c2+kzlGSb7xaRkqjPfnoH13b625Jkm1+IsneziCzv4NqU1HO3MMb0yQ2de7UBGAn4gc+AsTFlbgAec9KXAi+k0N5BwEQn3Q+d5Bxr7+nAP1JdtzE2bQYKOsmfjsZYFOAENHp9yu2OekZ2oJMEXVXPwGnARGBV1LnfAD920j8G7otzXT6w0dkPcNIDUmTv2YDPSd8Xz96uPENJtvlu4EddeG46/W1Jps0x+Q8AP3NTPXdn68stqLZo6saYANAaTT2a84HZTnoOMFkkNctdGmNKjTHLnHQtsBoNuNvbOR942iiLgTwRGZRqoxwmAxvMXtYgSwXGmHeB3TGno5/X2cA34lx6DrDQGLPbGFMJLASmJsxQh3j2GmNeN8YEncPFaLgz19BBHXeFrvy2JITObHZ+uy4GnkuGLcmgLwtUvGjqsT/4bWWcL1I1cFBSrOsEp6vxGOCjONknishnIrJARI5MqmHxMcDrIrLUiTofS1c+h1RxKR1/md1WzwADjTGloH9ogKI4Zdxa31fTwWoF7P0ZSjY3Od2SszroRnVrHZ8K7DQdh41zWz3vlb4sUF2Jpt7liOvJQkRygJeAW40xNTHZy9DuqPHAw8Dfk21fHE42xkwEpgE3ishpMfmuq2MAEfED5wEvxsl2Yz13FdfVt4jcga6a/WwHRfb2DCWTR4FRwAQ0BNsDccq4ro4dZtB568lN9dwl+rJAdSWaelsZ0eC2uexfk79HEJE0VJyeNcbMjc03xtQYY+qc9HwgTUQKkmxmrE3bnX0Z8DLa/RFNl6Lap4BpwDJjzM7YDDfWs8PO1u5RZ18Wp4yr6ttx0vg68G3jDITE0oVnKGkYY3YaY0LGmDDwpw5scVUdQ9vv14XACx2VcVM9d5W+LFBLgDEiMsL5t3wpMC+mzDyg1cvpImBRR1+iROP0Hz8JrDbGPNhBmeLWMTIROR79/CqSZ+Ue9mSLSL/WNDooviqm2DzgCseb7wSgurWbKsV0+G/TbfUcRfTzeiXwSpwy/wLOFpEBTvfU2c65pCMiU4HbgfOMMQ0dlOnKM5Q0YsZHL+jAlq78tiSbKcCXxpht8TLdVs9dJtVeGoncUA+ytajHzR3OuXvQLwxABtrFsx74GBiZQltPQbsJVgDLnW06cD1wvVPmJuBz1GtoMXBSiut3pGPLZ45drXUcbbMAjzifwUpgkgueiyxUcHKjzrmqnlHxLAVa0H/s16Djo28C65x9vlN2EvBE1LVXO8/0euCqFNq7Hh2raX2eWz1mBwPzO3uGUmjzX5zndAUqOoNibXaO9/htSZXNzvk/tz6/UWVdUc/d2WyoI4vFYrG4kr7cxWexWCyWXowVKIvFYrG4EitQFovFYnElVqAsFovF4kqsQFksFovFlViBslh6KU7U9X+k2g6LJVFYgbJYLBaLK7ECZbEkGBGZKSIfO+vw/FFEvCJSJyIPiMgyEXlTRAqdshNEZHHUGkoDnPOjReQNJ4DtMhEZ5dw+R0TmOOsuPZuqaPwWSyKwAmWxJBAROQK4BA3UOQEIAd8GstFYgBOBd4CfO5c8DdxujBmHRjRoPf8s8IjRALYnodEEQKPe3wqMRaMFnJzwN2WxJAlfqg2wWPo4k4FjgSVO4yYTDfIaJhLY8xlgrojkAnnGmHec87OBF50YakOMMS8DGGOaAJz7fWyc+GvOSqrDgfcT/7YslsRjBcpiSSwCzDbG/KTdSZG7Ysp1FnOss2675qh0CPudtvQhbBefxZJY3gQuEpEiABHJF5Fh6HfvIqfMZcD7xphqoFJETnXOXw68Y3RdsG0i8g3nHukikpXUd2GxpAD7b8tiSSDGmC9E5E50JVMPGoX6RqAeOFJElqIrOV/iXHIl8JgjQBuBq5zzlwN/FJF7nHt8K4lvw2JJCTaaucWSAkSkzhiTk2o7LBY3Y7v4LBaLxeJKbAvKYrFYLK7EtqAsFovF4kqsQFksFovFlViBslgsFosrsQJlsVgsFldiBcpisVgsruT/AdjjdbrbZ34IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the history\n",
    "_, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.plot(history.history['accuracy'])\n",
    "ax1.plot(history.history['val_accuracy'])\n",
    "ax1.set_title(f'model_accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['train', 'test'], loc='best')\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.set_title(f'model_loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['train', 'test'], loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Prediction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with the training phase, we are ready to make predictions.\n",
    "\n",
    "1) As we were saving the model's checkpoints, we will load the model locally from the drive. My selection would be the second last weights for no specific reason. We will use kera's, models' `load_model` method for the purpose. We define the path to the checkpoints of our choice and then we load the model.\n",
    "\n",
    "2) We still have our `test_generator` unused so we will make the predictions on the test generator via the method `predict_generator`. This will provide us with a list of softmax probability vector for each class. As we are only concerned with the class of max probability, we use the numpy's `argmax` method to select the heights probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T11:38:39.669287Z",
     "start_time": "2020-02-06T11:38:39.660662Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:37:18.080007Z",
     "start_time": "2020-02-02T13:37:16.877476Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_model = \"./checkpoints/weights-improvement-15-0.93.hdf5\"\n",
    "model = load_model(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:37:32.680941Z",
     "start_time": "2020-02-02T13:37:21.716237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predicions\n",
    "Y_pred = model.predict_generator(\n",
    "    test_generator, test_generator.n // test_generator.batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Model Evaluation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the predictions, we are ready to check our model's evaluation based on the predictions. For this we will use the `confusion_matrix` and `classification_report`.\n",
    "\n",
    "1) We simply pass the `test_generator.classes` and the `y_pred` of our model to the confusion matrix which generates the confusion matrix.\n",
    "\n",
    "2) We generate the classification report then. First we get the `target_names` using the `test_generator.class_indices`. Then we simply pass ont `test_generator.classes`, `y_pred`, `target_names` to the `classification_report` method from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:37:34.884503Z",
     "start_time": "2020-02-02T13:37:34.871636Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:37:38.977753Z",
     "start_time": "2020-02-02T13:37:38.950373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "[[153  31  23 113 117 155]\n",
      " [ 32   9   6  33  29  27]\n",
      " [ 11  10   1  13  11  11]\n",
      " [119  25  32 100 113 117]\n",
      " [126  32  23 101  85 137]\n",
      " [150  34  33 116 117 126]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix:\\n')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T13:37:51.473712Z",
     "start_time": "2020-02-02T13:37:51.453409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bag       0.26      0.26      0.26       592\n",
      "      Sandal       0.06      0.07      0.06       136\n",
      "  automobile       0.01      0.02      0.01        57\n",
      "        bird       0.21      0.20      0.20       506\n",
      "       truck       0.18      0.17      0.17       504\n",
      "  Ankle boot       0.22      0.22      0.22       576\n",
      "\n",
      "    accuracy                           0.20      2371\n",
      "   macro avg       0.16      0.15      0.16      2371\n",
      "weighted avg       0.21      0.20      0.20      2371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report:\\n')\n",
    "target_names = list(test_generator.class_indices.keys())\n",
    "print(classification_report(test_generator.classes,\n",
    "                            y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the task is\n",
    "\n",
    "  - We load the Json\n",
    "  - We Process the image based on the color mode\n",
    "  - We split the data in train-test\n",
    "  - We then compute the class weights\n",
    "  - We store the images locally\n",
    "  - We load the images using the Image data generator\n",
    "  - We get the train-validation and test generator\n",
    "  - We apply augmentation on the train and validation set.\n",
    "  - We train the custom model using the callback in which we define the check points and early stopping.\n",
    "  - We make predictions using the test generator\n",
    "  - We evaluate the model using the confusion matrix and classification report.\n",
    "  \n",
    "We see that the model is classifying just a considerable amount which can be resolved using the `hyper-parameter tunning`. We can use the grid search for this purpose and tune the parameters like `learning rate`, `loss` and `optimizers`. The problem is that the grid search will take some great amount of time. But the results can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for Arbisoft Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T15:45:07.874454Z",
     "start_time": "2020-02-06T15:45:07.860772Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from json import load\n",
    "from math import sqrt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T15:45:08.038416Z",
     "start_time": "2020-02-06T15:45:08.016579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the unique classes\n",
    "classes = np.array(listdir(\"./data/images/train/\"))\n",
    "\n",
    "\n",
    "def predict(path):\n",
    "    '''\n",
    "        This method assumes the data to be in a json file just like the dataset. \n",
    "        It will load a model locally that was stored after training and use it to predict the class.\n",
    "    '''\n",
    "\n",
    "    # Load the locally saved model\n",
    "    path_to_model = \"./checkpoints/weights-improvement-15-0.93.hdf5\"\n",
    "    model = load_model(path_to_model)\n",
    "\n",
    "    # Load the json data\n",
    "    with open(path) as json_file:\n",
    "        raw_data = load(json_file)\n",
    "\n",
    "    # For every dictionary in `raw_data`\n",
    "    for obj in raw_data:\n",
    "\n",
    "        # Get the Label of the Image\n",
    "        label = list(obj)[0]\n",
    "        # Get the Color_Mode of the image\n",
    "        color_mode = list(obj[label])[0]\n",
    "\n",
    "        # Case 1: If the image is gray\n",
    "        if color_mode == 'grey':\n",
    "\n",
    "            # Get the pixel List casted to numpy array\n",
    "            pixels = np.array(obj[label][color_mode])\n",
    "\n",
    "            # Reshape the 1D Array to a 2D array to form an image\n",
    "            reshape_index = int(sqrt(pixels.shape[0]))\n",
    "            image = np.reshape(pixels, (reshape_index, reshape_index))\n",
    "\n",
    "            # Resize\n",
    "            image = cv2.resize(image.astype('float32'), (30, 30))\n",
    "            # Expand the dims to make a Rank 4 Tensor\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = np.expand_dims(image, axis=3)\n",
    "\n",
    "        # Case 2: If the image is Colored\n",
    "        elif color_mode == 'R':\n",
    "\n",
    "            # Get Red Channel, cast to numpy array and Reshape 1D -> 2D\n",
    "            r_pixels = np.array(obj[label]['R'])\n",
    "            reshape_index = int(sqrt(r_pixels.shape[0]))\n",
    "            r_pixels = np.reshape(r_pixels, (reshape_index, reshape_index))\n",
    "\n",
    "            # Get Green Channel, cast to numpy array and Reshape 1D -> 2D\n",
    "            g_pixels = np.array(obj[label]['G'])\n",
    "            reshape_index = int(sqrt(g_pixels.shape[0]))\n",
    "            g_pixels = np.reshape(g_pixels, (reshape_index, reshape_index))\n",
    "\n",
    "            # Get Blue Channel, cast to numpy array and Reshape 1D -> 2D\n",
    "            b_pixels = np.array(obj[label]['B'])\n",
    "            reshape_index = int(sqrt(b_pixels.shape[0]))\n",
    "            b_pixels = np.reshape(b_pixels, (reshape_index, reshape_index))\n",
    "\n",
    "            # Combine the 3 channels\n",
    "            image = np.dstack((b_pixels, g_pixels, r_pixels))\n",
    "\n",
    "            # Resize\n",
    "            image = cv2.resize(image.astype('float32'), (30, 30))\n",
    "            # Change to grayscale\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # Expand the dims to make a Rank 4 Tensor\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = np.expand_dims(image, axis=3)\n",
    "\n",
    "        print(classes[np.argmax(model.predict(image), axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T15:45:08.778056Z",
     "start_time": "2020-02-06T15:45:08.766902Z"
    }
   },
   "outputs": [],
   "source": [
    "test = \"./data/gen_img_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T15:45:22.537054Z",
     "start_time": "2020-02-06T15:45:10.350466Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ankle boot']\n",
      "['Sandal']\n",
      "['Sandal']\n",
      "['Ankle boot']\n",
      "['Sandal']\n",
      "['Sandal']\n",
      "['Ankle boot']\n",
      "['Bag']\n",
      "['Bag']\n",
      "['Bag']\n",
      "['Sandal']\n",
      "['Ankle boot']\n",
      "['Sandal']\n",
      "['Ankle boot']\n",
      "['Bag']\n",
      "['Sandal']\n",
      "['Sandal']\n",
      "['Sandal']\n",
      "['Ankle boot']\n",
      "['Sandal']\n",
      "['Ankle boot']\n",
      "['Ankle boot']\n",
      "['Ankle boot']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-99847d487582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e6dbbb324f1e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    607\u001b[0m   \u001b[0;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 328\u001b[0;31m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1214\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3453\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3454\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3455\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3456\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1854\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2688\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mto_tensor_list\u001b[0;34m(element_spec, element)\u001b[0m\n\u001b[1;32m    371\u001b[0m   return _to_tensor_list_helper(\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m       element_spec, element)\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[0;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   return functools.reduce(\n\u001b[0;32m--> 322\u001b[0;31m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
